{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL-to-Text Model Evaluation\n",
    "\n",
    "This notebook evaluates pre-trained models for SQL-to-text generation without any training.\n",
    "\n",
    "## Features\n",
    "- Evaluate any HuggingFace model (seq2seq or causal LM)\n",
    "- Auto-detects model architecture\n",
    "- Multiple metrics: BLEU, ROUGE, CHRF, LaBSE similarity\n",
    "- Test on custom SQL queries\n",
    "- Save evaluation results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n!pip install -q torch transformers datasets sacremoses sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n\nSet the model and evaluation parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"cointegrated/rut5-base\"\n",
    "DATA_DIR = \".\"\n",
    "OUTPUT_FILE = \"evaluation_results.json\"\n",
    "NUM_SAMPLES = 50\n",
    "MAX_NEW_TOKENS = 100\n",
    "TEMPERATURE = 0.7\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Samples to evaluate: {NUM_SAMPLES if NUM_SAMPLES else 'All'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "print(\"Please upload pauq_dev.json\")\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List, Dict\n",
    "\n",
    "def load_pauq_data(data_dir: str, split: str = \"dev\") -> List[Dict]:\n",
    "    filename = f\"pauq_{split}.json\"\n",
    "    filepath = os.path.join(data_dir, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"Data file not found: {filepath}\")\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"Loaded {len(data)} examples from {filename}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_subset(data: List[Dict], num_samples: int = None) -> List[Dict]:\n",
    "    if num_samples:\n",
    "        return data[:num_samples]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = load_pauq_data(DATA_DIR, \"dev\")\n",
    "eval_data = get_eval_subset(dev_data, NUM_SAMPLES)\n",
    "print(f\"\\nEvaluating on {len(eval_data)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def load_model_and_tokenizer(model_name: str):\n",
    "    from transformers import AutoTokenizer\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    try:\n",
    "        from transformers import AutoModelForSeq2SeqLM\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        is_seq2seq = True\n",
    "        print(\"Detected: Seq2Seq model\")\n",
    "    except (OSError, ValueError, KeyError):\n",
    "        from transformers import AutoModelForCausalLM\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        is_seq2seq = False\n",
    "        print(\"Detected: Causal LM\")\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    return model, tokenizer, is_seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer, is_seq2seq = load_model_and_tokenizer(MODEL_NAME)\n",
    "print(f\"Model loaded on: {model.device}\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question(model, tokenizer, sql_query: str, is_seq2seq: bool,\n",
    "                      max_new_tokens: int = 100, temperature: float = 0.7):\n",
    "    if is_seq2seq:\n",
    "        prompt = f\"SQL: {sql_query}\"\n",
    "    else:\n",
    "        prompt = f\"SQL: {sql_query}\\nQuestion:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "    if is_seq2seq:\n",
    "        result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    else:\n",
    "        full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        result = full_output.split(\"Question:\")[-1].strip()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Evaluation\n\nTest the model on a few samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSample predictions:\")\n",
    "print(\"=\" * 80)\n",
    "for i, item in enumerate(eval_data[:5]):\n",
    "    sql_query = item.get(\"query\", {}).get(\"en\", \"\")\n",
    "    actual_question = item.get(\"question\", {}).get(\"en\", \"\")\n",
    "    predicted_question = generate_question(\n",
    "        model, tokenizer, sql_query, is_seq2seq, MAX_NEW_TOKENS, TEMPERATURE\n",
    "    )\n",
    "    print(f\"\\n--- Sample {i+1} ---\")\n",
    "    print(f\"SQL: {sql_query}\")\n",
    "    print(f\"Expected: {actual_question}\")\n",
    "    print(f\"Predicted: {predicted_question}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions\n\nFunctions to compute metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(references, hypotheses):\n",
    "    from sacrebleu.metrics import BLEU, CHRF\n",
    "    from rouge_score import rouge_scorer\n",
    "    from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "    print(\"Computing BLEU...\")\n",
    "    bleu_metric = BLEU()\n",
    "    bleu_result = bleu_metric.corpus_score(hypotheses, [references])\n",
    "\n",
    "    print(\"Computing ROUGE...\")\n",
    "    rouge_scorer_instance = rouge_scorer.RougeScorer(\n",
    "        ['rouge1', 'rouge2', 'rougeL'], use_stemmer=True\n",
    "    )\n",
    "    rouge1_scores, rouge2_scores, rougeL_scores = [], [], []\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        scores = rouge_scorer_instance.score(ref, hyp)\n",
    "        rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "        rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "        rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "    print(\"Computing CHRF...\")\n",
    "    chrf_metric = CHRF()\n",
    "    chrf_result = chrf_metric.corpus_score(hypotheses, [references])\n",
    "\n",
    "    print(\"Computing LaBSE similarity...\")\n",
    "    labse_model = SentenceTransformer('sentence-transformers/LaBSE')\n",
    "    ref_embeddings = labse_model.encode(references, convert_to_tensor=True)\n",
    "    hyp_embeddings = labse_model.encode(hypotheses, convert_to_tensor=True)\n",
    "    similarities = util.cos_sim(hyp_embeddings, ref_embeddings)\n",
    "    similarity_scores = torch.diagonal(similarities).cpu().numpy()\n",
    "\n",
    "    return {\n",
    "        'BLEU-1': bleu_result.precisions[0],\n",
    "        'BLEU-2': bleu_result.precisions[1],\n",
    "        'BLEU-3': bleu_result.precisions[2],\n",
    "        'BLEU-4': bleu_result.precisions[3],\n",
    "        'ROUGE-1': np.mean(rouge1_scores) * 100,\n",
    "        'ROUGE-2': np.mean(rouge2_scores) * 100,\n",
    "        'ROUGE-L': np.mean(rougeL_scores) * 100,\n",
    "        'CHRF': chrf_result.score,\n",
    "        'LaBSE-Similarity': np.mean(similarity_scores) * 100,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(model, tokenizer, eval_data, is_seq2seq,\n",
    "                   max_new_tokens, temperature, output_file=None):\n",
    "    references, hypotheses, predictions = [], [], []\n",
    "    print(f\"\\nEvaluating {len(eval_data)} samples...\")\n",
    "\n",
    "    for item in eval_data:\n",
    "        sql_query = item.get(\"query\", {}).get(\"en\", \"\")\n",
    "        actual_question = item.get(\"question\", {}).get(\"en\", \"\")\n",
    "        predicted = generate_question(\n",
    "            model, tokenizer, sql_query, is_seq2seq, max_new_tokens, temperature\n",
    "        )\n",
    "        references.append(actual_question)\n",
    "        hypotheses.append(predicted)\n",
    "        predictions.append({\n",
    "            'id': item.get('id', ''),\n",
    "            'sql': sql_query,\n",
    "            'expected': actual_question,\n",
    "            'predicted': predicted,\n",
    "        })\n",
    "\n",
    "    print(\"Computing metrics...\")\n",
    "    metrics = compute_metrics(references, hypotheses)\n",
    "\n",
    "    if output_file:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(predictions, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"\\nPredictions saved to {output_file}\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Full Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = run_evaluation(\n",
    "    model, tokenizer, eval_data, is_seq2seq,\n",
    "    MAX_NEW_TOKENS, TEMPERATURE, OUTPUT_FILE\n",
    ")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nModel: {MODEL_NAME}\")\n",
    "print(f\"Dataset: {len(eval_data)} samples\")\n",
    "print(\"\\n--- Metrics ---\")\n",
    "for key, value in metrics.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom SQL Test\n\nTest with your own SQL queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sqls = [\n",
    "    \"SELECT name FROM users WHERE age > 25;\",\n",
    "    \"SELECT COUNT(*) FROM orders WHERE status = 'completed';\",\n",
    "    \"SELECT product, SUM(quantity) FROM sales GROUP BY product ORDER BY SUM(quantity) DESC;\",\n",
    "]\n",
    "print(\"\\nCustom SQL Tests:\")\n",
    "print(\"=\" * 80)\n",
    "for i, test_sql in enumerate(test_sqls):\n",
    "    predicted = generate_question(\n",
    "        model, tokenizer, test_sql, is_seq2seq, MAX_NEW_TOKENS, TEMPERATURE\n",
    "    )\n",
    "    print(f\"\\n--- Test {i+1} ---\")\n",
    "    print(f\"SQL: {test_sql}\")\n",
    "    print(f\"Generated Question: {predicted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Multiple Models\n\nDefine a list of models to compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_TO_COMPARE = [\n",
    "    \"cointegrated/rut5-base\",\n",
    "    \"google/flan-t5-base\",\n",
    "    \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "]\n",
    "COMPARE_NUM_SAMPLES = 20\n",
    "compare_data = get_eval_subset(dev_data, COMPARE_NUM_SAMPLES)\n",
    "print(f\"Comparing {len(MODELS_TO_COMPARE)} models on {len(compare_data)} samples...\")\n",
    "all_results = []\n",
    "\n",
    "for model_name in MODELS_TO_COMPARE:\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"Evaluating: {model_name}\")\n",
    "    try:\n",
    "        compare_model, compare_tokenizer, compare_is_seq2seq = load_model_and_tokenizer(model_name)\n",
    "        compare_metrics = run_evaluation(\n",
    "            compare_model, compare_tokenizer, compare_data, compare_is_seq2seq,\n",
    "            MAX_NEW_TOKENS, TEMPERATURE, None\n",
    "        )\n",
    "        result = {'model': model_name}\n",
    "        result.update(compare_metrics)\n",
    "        all_results.append(result)\n",
    "        del compare_model, compare_tokenizer\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating {model_name}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_results = pd.DataFrame(all_results)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL COMPARISON RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(df_results.to_string(index=False))\n",
    "df_results.to_csv('model_comparison.csv', index=False)\n",
    "print(\"\\nResults saved to model_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Results (Optional)\n\nDownload the evaluation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStarting download...\")\n",
    "files.download(OUTPUT_FILE)\n",
    "if os.path.exists('model_comparison.csv'):\n",
    "    files.download('model_comparison.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}