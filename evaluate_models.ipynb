{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL-to-Text Model Evaluation\n",
    "\n",
    "This notebook evaluates pre-trained models for SQL-to-text generation without any training.\n",
    "\n",
    "## Features\n",
    "- Evaluate any HuggingFace model (seq2seq or causal LM)\n",
    "- Model-specific batch sizes based on memory requirements\n",
    "- Batched generation for efficient GPU utilization\n",
    "- GPU memory monitoring\n",
    "- Multiple metrics: BLEU, ROUGE, CHRF, LaBSE similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies\n!pip install -q torch transformers datasets sacremoses sentence-transformers sacrebleu rouge-score"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set the model and evaluation parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"cointegrated/rut5-base\"\n",
    "DATA_DIR = \".\"\n",
    "OUTPUT_FILE = \"evaluation_results.json\"\n",
    "NUM_SAMPLES = None  # Use full dataset\n",
    "MAX_NEW_TOKENS = 100\n",
    "TEMPERATURE = 0.7\n",
    "\n",
    "# Model-specific batch sizes (adjust based on GPU memory)\n",
    "MODEL_BATCH_SIZES = {\n",
    "    \"cointegrated/rut5-base\": 256,\n",
    "    \"cointegrated/rut5-small\": 256,\n",
    "    \"google/flan-t5-base\": 128,\n",
    "    \"google/flan-t5-small\": 256,\n",
    "    \"google/flan-t5-large\": 64,\n",
    "    \"facebook/bart-base\": 128,\n",
    "    \"facebook/bart-large\": 32,\n",
    "    \"Qwen/Qwen2.5-0.5B-Instruct\": 256,\n",
    "    \"Qwen/Qwen2.5-1.5B-Instruct\": 128,\n",
    "    \"Qwen/Qwen2.5-3B-Instruct\": 64,\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\": 32,\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\": 256,\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\": 128,\n",
    "    \"meta-llama/Llama-3.2-8B-Instruct\": 64,\n",
    "}\n",
    "\n",
    "def get_batch_size(model_name: str, default_size: int = 128) -> int:\n",
    "    \"\"\"Get batch size for model, with fallback to default.\"\"\"\n",
    "    return MODEL_BATCH_SIZES.get(model_name, default_size)\n",
    "\n",
    "# Get batch size for selected model\n",
    "BATCH_SIZE = get_batch_size(MODEL_NAME, 128)\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Samples to evaluate: All\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "print(\"Please upload pauq_dev.json\")\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Memory Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"GPU Memory Check\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "    free = total_memory - allocated\n",
    "    \n",
    "    print(f\"Total Memory: {total_memory:.2f} GB\")\n",
    "    print(f\"Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"Reserved: {reserved:.2f} GB\")\n",
    "    print(f\"Available: {free:.2f} GB\")\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"No GPU available\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List, Dict\n",
    "\n",
    "def load_pauq_data(data_dir: str, split: str = \"dev\") -> List[Dict]:\n",
    "    filename = f\"pauq_{split}.json\"\n",
    "    filepath = os.path.join(data_dir, filename)\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"Data file not found: {filepath}\")\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    print(f\"Loaded {len(data)} examples from {filename}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_subset(data: List[Dict], num_samples: int = None) -> List[Dict]:\n",
    "    if num_samples:\n",
    "        return data[:num_samples]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = load_pauq_data(DATA_DIR, \"dev\")\n",
    "eval_data = get_eval_subset(dev_data, NUM_SAMPLES)\n",
    "print(f\"\\nEvaluating on {len(eval_data)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def load_model_and_tokenizer(model_name: str):\n",
    "    from transformers import AutoTokenizer\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    try:\n",
    "        from transformers import AutoModelForSeq2SeqLM\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        is_seq2seq = True\n",
    "        print(\"Detected: Seq2Seq model\")\n",
    "    except (OSError, ValueError, KeyError):\n",
    "        from transformers import AutoModelForCausalLM\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        is_seq2seq = False\n",
    "        print(\"Detected: Causal LM\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load model {model_name}: {e}\")\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer, is_seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer, is_seq2seq = load_model_and_tokenizer(MODEL_NAME)\n",
    "print(f\"Model loaded on: {model.device}\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batched Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions_batched(model, tokenizer, sql_queries: List[str], is_seq2seq: bool,\n",
    "                                   max_new_tokens: int = 100, temperature: float = 0.7):\n",
    "    \"\"\"Generate questions from SQL queries in batches for efficient GPU usage.\"\"\"\n",
    "    if is_seq2seq:\n",
    "        prompts = [f\"SQL: {sql}\" for sql in sql_queries]\n",
    "        inputs = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "        # Decode all outputs\n",
    "        results = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    else:\n",
    "        # Causal LM: need to handle continuation format\n",
    "        prompts = [f\"SQL: {sql}\\nQuestion:\" for sql in sql_queries]\n",
    "        inputs = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "        # Decode and extract continuation after \"Question:\"\n",
    "        full_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        results = [output.split(\"Question:\")[-1].strip() for output in full_outputs]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Evaluation\n",
    "\n",
    "Test the model on a few samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSample predictions:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get sample data\n",
    "sample_data = eval_data[:5]\n",
    "sample_sqls = [item.get(\"query\", {}).get(\"en\", \"\") for item in sample_data]\n",
    "\n",
    "# Generate in batch\n",
    "predictions = generate_questions_batched(\n",
    "    model, tokenizer, sample_sqls, is_seq2seq, MAX_NEW_TOKENS, TEMPERATURE\n",
    ")\n",
    "\n",
    "for i, (item, predicted) in enumerate(zip(sample_data, predictions)):\n",
    "    sql_query = item.get(\"query\", {}).get(\"en\", \"\")\n",
    "    actual_question = item.get(\"question\", {}).get(\"en\", \"\")\n",
    "    print(f\"\\n--- Sample {i+1} ---\")\n",
    "    print(f\"SQL: {sql_query}\")\n",
    "    print(f\"Expected: {actual_question}\")\n",
    "    print(f\"Predicted: {predicted}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions\n",
    "\n",
    "Functions to compute metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# LaBSE Model Caching\n",
    "labse_model = None\n",
    "\n",
    "def get_labse_model():\n",
    "    global labse_model\n",
    "    if labse_model is None:\n",
    "        print(\"Loading LaBSE model...\")\n",
    "        labse_model = SentenceTransformer('sentence-transformers/LaBSE')\n",
    "    return labse_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(references, hypotheses):\n",
    "    from sacrebleu.metrics import BLEU, CHRF\n",
    "    import torch\n",
    "    from rouge_score import rouge_scorer\n",
    "    from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "    print(\"Computing BLEU...\")\n",
    "    bleu_metric = BLEU()\n",
    "    bleu_result = bleu_metric.corpus_score(hypotheses, [references])\n",
    "\n",
    "    print(\"Computing ROUGE...\")\n",
    "    rouge_scorer_instance = rouge_scorer.RougeScorer(\n",
    "        ['rouge1', 'rouge2', 'rougeL'], use_stemmer=True\n",
    "    )\n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        scores = rouge_scorer_instance.score(ref, hyp)\n",
    "        rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "        rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "        rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "    print(\"Computing CHRF...\")\n",
    "    chrf_metric = CHRF()\n",
    "    chrf_result = chrf_metric.corpus_score(hypotheses, [references])\n",
    "\n",
    "    # LaBSE similarity - use cached model\n",
    "    labse = get_labse_model()\n",
    "    ref_embeddings = labse.encode(references, convert_to_tensor=True)\n",
    "    hyp_embeddings = labse.encode(hypotheses, convert_to_tensor=True)\n",
    "    similarities = util.cos_sim(hyp_embeddings, ref_embeddings)\n",
    "    similarity_scores = torch.diagonal(similarities).cpu().numpy()\n",
    "\n",
    "    return {\n",
    "        'BLEU-1': bleu_result.precisions[0],\n",
    "        'BLEU-2': bleu_result.precisions[1],\n",
    "        'BLEU-3': bleu_result.precisions[2],\n",
    "        'BLEU-4': bleu_result.precisions[3],\n ",
    "        'ROUGE-1': np.mean(rouge1_scores) * 100,\n",
    "        'ROUGE-2': np.mean(rouge2_scores) * 100,\n ",
    "        'ROUGE-L': np.mean(rougeL_scores) * 100,\n ",
    "        'CHRF': chrf_result.score,\n ",
    "        'LaBSE-Similarity': np.mean(similarity_scores) * 100,\n ",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation_batched(model, tokenizer, eval_data, is_seq2seq,\n ",
    "                                  batch_size, max_new_tokens, temperature, output_file=None):\n ",
    "    references, hypotheses, predictions = [], [], []\n ",
    "    num_samples = len(eval_data)\n ",
    "    num_batches = (num_samples + batch_size - 1) // batch_size\n ",
    "    print(f\"\\nEvaluating {num_samples} samples in {num_batches} batches (size={batch_size})...\")\n",
    "\n ",
    "    for batch_idx in range(num_batches):\n ",
    "        start_idx = batch_idx * batch_size\n ",
    "        end_idx = min(start_idx + batch_size, num_samples)\n ",
    "        batch_data = eval_data[start_idx:end_idx]\n ",
    "\n ",
    "        # Prepare batch inputs\n ",
    "        batch_sqls = [item.get(\"query\", {}).get(\"en\", \"\") for item in batch_data]\n ",
    "        batch_refs = [item.get(\"question\", {}).get(\"en\", \"\") for item in batch_data]\n ",
    "\n ",
    "        # Generate in batch\n ",
    "        batch_preds = generate_questions_batched(\n ",
    "            model, tokenizer, batch_sqls, is_seq2seq, max_new_tokens, temperature\n ",
    "        )\n ",
    "\n ",
    "        # Store results\n ",
    "        for item, ref, pred in zip(batch_data, batch_refs, batch_preds):\n ",
    "            references.append(ref)\n ",
    "            hypotheses.append(pred)\n ",
    "            predictions.append({\n ",
    "                'id': item.get('id', ''),\n ",
    "                'sql': item.get('query', {}).get('en', ''),\n ",
    "                'expected': ref,\n ",
    "                'predicted': pred,\n ",
    "            })\n ",
    "\n ",
    "        print(f\"Processed batch {batch_idx + 1}/{num_batches} ({end_idx}/{num_samples} samples)\")\n ",
    "\n ",
    "    print(\"Computing metrics...\")\n ",
    "    metrics = compute_metrics(references, hypotheses)\n ",
    "\n ",
    "    if output_file:\n ",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n ",
    "            json.dump(predictions, f, indent=2, ensure_ascii=False)\n ",
    "        print(f\"\\nPredictions saved to {output_file}\")\n ",
    "\n ",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Full Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "metrics = run_evaluation_batched(\n    model, tokenizer, eval_data, is_seq2seq,\n    BATCH_SIZE, MAX_NEW_TOKENS, TEMPERATURE, OUTPUT_FILE\n)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"EVALUATION RESULTS\")\nprint(\"=\" * 80)\nprint(f\"\\nModel: {MODEL_NAME}\")\nprint(f\"Dataset: {len(eval_data)} samples\")\nprint(f\"Batch size: {BATCH_SIZE}\")\nprint(\"\\n--- Metrics ---\")\nfor key, value in metrics.items():\n    print(f\"{key}: {value:.4f}\")\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom SQL Test\n",
    "\n",
    "Test with your own SQL queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sqls = [\n ",
    "    \"SELECT name FROM users WHERE age > 25;\",\n ",
    "    \"SELECT COUNT(*) FROM orders WHERE status = 'completed';\",\n ",
    "    \"SELECT product, SUM(quantity) FROM sales GROUP BY product ORDER BY SUM(quantity) DESC;\",\n ",
    "]\n ",
    "print(\"\\nCustom SQL Tests:\")\n ",
    "print(\"=\" * 80)\n ",
    "\n ",
    "# Generate in batch\n ",
    "predictions = generate_questions_batched(\n ",
    "    model, tokenizer, test_sqls, is_seq2seq, MAX_NEW_TOKENS, TEMPERATURE\n ",
    ")\n ",
    "\n ",
    "for i, (test_sql, predicted) in enumerate(zip(test_sqls, predictions)):\n ",
    "    print(f\"\\n--- Test {i+1} ---\")\n ",
    "    print(f\"SQL: {test_sql}\")\n ",
    "    print(f\"Generated Question: {predicted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Multiple Models\n",
    "\n",
    "Define a list of models to compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_TO_COMPARE = [\n ",
    "    \"cointegrated/rut5-base\",\n ",
    "    \"google/flan-t5-base\",\n ",
    "    \"Qwen/Qwen2.5-0.5B-Instruct\",\n ",
    "]\n ",
    "\n ",
    "COMPARE_NUM_SAMPLES = None  # Use full dataset\n",
    "compare_data = get_eval_subset(dev_data, COMPARE_NUM_SAMPLES)\n ",
    "print(f\"\\nComparing {len(MODELS_TO_COMPARE)} models on {len(compare_data)} samples...\")\n ",
    "all_results = []\n ",
    "\n ",
    "for model_name in MODELS_TO_COMPARE:\n ",
    "    print(f\"\\n\" + \"=\" * 80)\n ",
    "    print(f\"Evaluating: {model_name}\")\n ",
    "    print(f\"=\" * 80)\n ",
    "    try:\n ",
    "        compare_model, compare_tokenizer, compare_is_seq2seq = load_model_and_tokenizer(model_name)\n ",
    "        compare_batch_size = get_batch_size(model_name, 128)\n ",
    "        compare_metrics = run_evaluation_batched(\n ",
    "            compare_model, compare_tokenizer, compare_data, compare_is_seq2seq,\n ",
    "            compare_batch_size, MAX_NEW_TOKENS, TEMPERATURE, None\n ",
    "        )\n ",
    "        result = {\"model\": model_name}\n ",
    "        result.update(compare_metrics)\n ",
    "        all_results.append(result)\n ",
    "        del compare_model, compare_tokenizer\n ",
    "        torch.cuda.empty_cache()\n ",
    "    except Exception as e:\n ",
    "        print(f\"Error evaluating {model_name}: {e}\")\n ",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n ",
    "\n ",
    "df_results = pd.DataFrame(all_results)\n ",
    "print(\"\\n\" + \"=\" * 80)\n ",
    "print(\"MODEL COMPARISON RESULTS\")\n ",
    "print(\"=\" * 80)\n ",
    "print(df_results.to_string(index=False))\n ",
    "df_results.to_csv('model_comparison.csv', index=False)\n ",
    "print(\"\\nResults saved to model_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Results (Optional)\n",
    "\n",
    "Download evaluation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStarting download...\")\n ",
    "files.download(OUTPUT_FILE)\n ",
    "if os.path.exists('model_comparison.csv'):\n ",
    "    files.download('model_comparison.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}