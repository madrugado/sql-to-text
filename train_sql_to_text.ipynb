{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL-to-Text Training with Qwen2.5-0.5B\n",
    "\n",
    "This notebook fine-tunes Qwen2.5-0.5B-Instruct to generate natural language questions from SQL queries using the PAUQ dataset.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies\n!pip install -q torch transformers datasets accelerate sacremoses sentence-transformers"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mount Google Drive (Optional)\n",
    "\n",
    "If you want to save your model to Google Drive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your Google Drive path for saving the model\n",
    "# OUTPUT_DIR = \"/content/drive/MyDrive/sql_to_text_model\"\n",
    "# Or use local Colab storage:\n",
    "OUTPUT_DIR = \"./sql_to_text_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Data\n",
    "\n",
    "Upload your `pauq_train.json` and `pauq_dev.json` files to the Colab runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data files\n",
    "from google.colab import files\n",
    "print(\"Please upload pauq_train.json and pauq_dev.json\")\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU available, using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "DATA_DIR = \".\"  # Current directory where data files were uploaded\n",
    "\n",
    "# Training configuration - adjust based on your GPU memory\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 8  # Increase if GPU memory allows (Colab T4: 8-16, A100: 16-32)\n",
    "GRADIENT_ACCUMULATION_STEPS = 2  # Decrease if batch_size is larger\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 2e-4\n",
    "WARMUP_STEPS = 100\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Gradient accumulation steps: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def load_pauq_data(data_dir, split=\"train\"):\n",
    "    \"\"\"Load PAUQ dataset from JSON file.\"\"\"\n",
    "    filename = f\"pauq_{split}.json\"\n",
    "    filepath = os.path.join(data_dir, filename)\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"Data file not found: {filepath}\")\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    print(f\"Loaded {len(data)} examples from {filename}\")\n",
    "    return data\n",
    "\n",
    "def prepare_training_data(data):\n",
    "    \"\"\"Prepare data for fine-tuning.\"\"\"\n",
    "    prepared = []\n",
    "\n",
    "    for item in data:\n",
    "        sql_query = item.get(\"query\", {}).get(\"en\", \"\")\n",
    "        question = item.get(\"question\", {}).get(\"en\", \"\")\n",
    "\n",
    "        if not sql_query or not question:\n",
    "            continue\n",
    "\n",
    "        sql_query = sql_query.strip()\n",
    "        question = question.strip()\n",
    "\n",
    "        # Format for instruction tuning\n",
    "        formatted_text = f\"SQL: {sql_query}\\nQuestion: {question}\"\n",
    "        prepared.append({\"text\": formatted_text})\n",
    "\n",
    "    print(f\"Prepared {len(prepared)} training examples\")\n",
    "    return prepared\n",
    "\n",
    "def tokenize_function(examples, tokenizer, max_length):\n",
    "    \"\"\"Tokenize the text data.\"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    return tokenized\n",
    "\n",
    "# Load data\n",
    "print(\"Loading training data...\")\n",
    "train_data = load_pauq_data(DATA_DIR, \"train\")\n",
    "train_prepared = prepare_training_data(train_data)\n",
    "\n",
    "print(\"\\nLoading validation data...\")\n",
    "val_data = load_pauq_data(DATA_DIR, \"dev\")\n",
    "val_prepared = prepare_training_data(val_data)\n",
    "\n",
    "# Create HuggingFace datasets\n",
    "train_dataset = Dataset.from_list(train_prepared)\n",
    "val_dataset = Dataset.from_list(val_prepared)\n",
    "\n",
    "print(f\"\\nTrain dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Set pad token if not exists\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Model loaded on: {model.device}\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tokenizing datasets...\")\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda x: tokenize_function(x, tokenizer, MAX_LENGTH),\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    ")\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "    lambda x: tokenize_function(x, tokenizer, MAX_LENGTH),\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    ")\n",
    "\n",
    "print(\"Tokenization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "# Calculate total training steps\n",
    "total_steps = (len(train_dataset) // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)) * NUM_EPOCHS\n",
    "print(f\"Total training steps: {total_steps}\")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    save_total_limit=3,\n",
    "    fp16=False,\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    eval_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Trainer ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "This will take some time. On Colab T4, expect ~20-30 minutes. On A100, ~5-10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Saving model to {OUTPUT_DIR}\")\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "print(f\"Loading model from {OUTPUT_DIR}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    OUTPUT_DIR,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Model loaded for evaluation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate predictions for samples\nnum_samples = 10\n\nprint(f\"\\nGenerating predictions for {num_samples} samples...\")\nprint(\"=\" * 80)\n\nfor i, item in enumerate(val_data[:num_samples]):\n    sql_query = item.get(\"query\", {}).get(\"en\", \"\")\n    actual_question = item.get(\"question\", {}).get(\"en\", \"\")\n\n    # Format input\n    prompt = f\"SQL: {sql_query}\\nQuestion:\"\n\n    # Generate\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=100,\n        temperature=0.7,\n        do_sample=True,\n        pad_token_id=tokenizer.pad_token_id,\n    )\n\n    # Decode prediction\n    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    predicted_question = full_output.split(\"Question:\")[-1].strip()\n\n    print(f\"\\n--- Sample {i+1} ---\")\n    print(f\"SQL: {sql_query}\")\n    print(f\"Expected: {actual_question}\")\n    print(f\"Predicted: {predicted_question}\")\n    print(\"-\" * 80)"
  },
  {
   "cell_type": "code",
   "source": "from sacrebleu.metrics import CHRF\nfrom sentence_transformers import SentenceTransformer, util\nimport numpy as np\nfrom tqdm import tqdm\n\nprint(\"Loading evaluation models...\")\n# Load chrF metric\nchrf_metric = CHRF()\n\n# Load LaBSE model for semantic similarity\nlabse_model = SentenceTransformer('sentence-transformers/LaBSE')\nprint(\"Evaluation models loaded!\")\n\n# Generate predictions for a subset of validation data\neval_subset_size = min(500, len(val_data))  # Use 500 samples or all if less\nval_subset = val_data[:eval_subset_size]\n\nprint(f\"\\nGenerating predictions for {eval_subset_size} validation samples...\")\n\nreferences = []\nhypotheses = []\n\nfor item in tqdm(val_subset, desc=\"Generating\"):\n    sql_query = item.get(\"query\", {}).get(\"en\", \"\")\n    actual_question = item.get(\"question\", {}).get(\"en\", \"\")\n\n    # Format input\n    prompt = f\"SQL: {sql_query}\\nQuestion:\"\n\n    # Generate\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=100,\n            temperature=0.7,\n            do_sample=True,\n            pad_token_id=tokenizer.pad_token_id,\n        )\n\n    # Decode prediction\n    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    predicted_question = full_output.split(\"Question:\")[-1].strip()\n\n    references.append(actual_question)\n    hypotheses.append(predicted_question)\n\nprint(\"\\nComputing chrF score...\")\n# Compute chrF score\nchrf_result = chrf_metric.corpus_score(hypotheses, [references])\n\nprint(\"\\nComputing LaBSE semantic similarity...\")\n# Compute LaBSE semantic similarity\nref_embeddings = labse_model.encode(references, convert_to_tensor=True)\nhyp_embeddings = labse_model.encode(hypotheses, convert_to_tensor=True)\n\n# Compute cosine similarity for each pair\nsimilarities = util.cos_sim(hyp_embeddings, ref_embeddings)\n# Take diagonal for reference-hypothesis pairs\nsimilarity_scores = torch.diagonal(similarities).cpu().numpy()\nmean_labse = np.mean(similarity_scores)\n\n# Print results\nprint(\"\\n\" + \"=\" * 80)\nprint(\"EVALUATION RESULTS\")\nprint(\"=\" * 80)\nprint(f\"\\nDataset size: {eval_subset_size} samples\")\nprint(f\"\\nchrF Score:\")\nprint(f\"  {chrf_result.format(width=2)}\")\nprint(f\"\\nLaBSE Semantic Similarity:\")\nprint(f\"  Mean Cosine Similarity: {mean_labse:.4f}\")\nprint(f\"  Std Dev: {np.std(similarity_scores):.4f}\")\nprint(f\"  Min: {np.min(similarity_scores):.4f}\")\nprint(f\"  Max: {np.max(similarity_scores):.4f}\")\nprint(\"=\" * 80)\n\n# Optional: Show distribution of scores\nprint(\"\\nLaBSE Score Distribution:\")\nprint(f\"  0-0.2:   {np.sum(similarity_scores < 0.2)} samples\")\nprint(f\"  0.2-0.4: {np.sum((similarity_scores >= 0.2) & (similarity_scores < 0.4))} samples\")\nprint(f\"  0.4-0.6: {np.sum((similarity_scores >= 0.4) & (similarity_scores < 0.6))} samples\")\nprint(f\"  0.6-0.8: {np.sum((similarity_scores >= 0.6) & (similarity_scores < 0.8))} samples\")\nprint(f\"  0.8-1.0: {np.sum(similarity_scores >= 0.8)} samples\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Evaluation Metrics (chrF and LaBSE)\n\nEvaluate the model using chrF (character-level F-score) and LaBSE (semantic similarity) scores across the validation set.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Custom SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with your own SQL queries\n",
    "test_sql = \"SELECT name, age FROM users WHERE age > 25 ORDER BY name;\"\n",
    "\n",
    "prompt = f\"SQL: {test_sql}\\nQuestion:\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "predicted_question = full_output.split(\"Question:\")[-1].strip()\n",
    "\n",
    "print(f\"SQL: {test_sql}\")\n",
    "print(f\"Generated Question: {predicted_question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Model (Optional)\n",
    "\n",
    "If you want to download the trained model to your local machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the model directory\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "zip_filename = \"sql_to_text_model.zip\"\n",
    "print(f\"Zipping model to {zip_filename}...\")\n",
    "shutil.make_archive(zip_filename.replace('.zip', ''), 'zip', OUTPUT_DIR)\n",
    "print(\"Zipping complete!\")\n",
    "\n",
    "# Download the zip file\n",
    "print(\"\\nStarting download...\")\n",
    "files.download(zip_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}