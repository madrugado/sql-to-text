{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL-to-Text Training Notebook\n",
    "\n",
    "This notebook fine-tunes models (both seq2seq and causal LMs) to generate natural language questions from SQL queries using the PAUQ dataset.\n",
    "\n",
    "## Features\n",
    "- Model-agnostic: Supports T5, RuT5, BART, Qwen, Llama, and more\n",
    "- Auto-detects model architecture (seq2seq vs causal LM)\n",
    "- Train/validation/test split functionality\n",
    "- Evaluation with multiple metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies\n!pip install -q torch transformers datasets accelerate sacremoses sentence-transformers sacrebleu rouge-score"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set the model and training parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Model configuration - Change to any HuggingFace model\n# Seq2Seq models: cointegrated/rut5-base, google/flan-t5-base, facebook/bart-base\n# Causal LMs: Qwen/Qwen2.5-0.5B-Instruct, meta-llama/Llama-3.2-1B-Instruct\nMODEL_NAME = \"cointegrated/rut5-base\"\n\n# Paths - using VM file system\nDATA_DIR = \".\"  # Current directory where data files were uploaded\nOUTPUT_DIR = \"./sql_to_text_model\"\n\n# Data splitting mode\n# True: Split train into train/val, use dev as test\n# False: Use train as train, dev as val (no test set)\nUSE_DEV_AS_TEST = True\nVAL_SPLIT_RATIO = 0.2  # 20% of train becomes validation\n\n# Training configuration\nMAX_LENGTH = 512\nBATCH_SIZE = 4\nGRADIENT_ACCUMULATION_STEPS = 4\nNUM_EPOCHS = 3\nLEARNING_RATE = 2e-4\n\nprint(f\"Model: {MODEL_NAME}\")\nprint(f\"Batch size: {BATCH_SIZE}\")\nprint(f\"Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"Use dev as test: {USE_DEV_AS_TEST}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Data\n",
    "\n",
    "Upload your `pauq_train.json` and `pauq_dev.json` files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from google.colab import files\nprint(\"Please upload pauq_train.json and pauq_dev.json\")\nuploaded = files.upload()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading Functions\n",
    "\n",
    "Functions to load and split the PAUQ dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport os\nfrom typing import List, Dict, Tuple\nimport random\n\ndef load_pauq_data(data_dir: str, split: str = \"train\") -> List[Dict]:\n    \"\"\"Load PAUQ dataset from JSON file.\"\"\"\n    filename = f\"pauq_{split}.json\"\n    filepath = os.path.join(data_dir, filename)\n\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f\"Data file not found: {filepath}\")\n\n    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n\n    print(f\"Loaded {len(data)} examples from {filename}\")\n    return data\n\ndef split_train_data(train_data: List[Dict], val_ratio: float = 0.2, seed: int = 42) -> Tuple[List[Dict], List[Dict]]:\n    \"\"\"Split training data into train and validation sets.\"\"\"\n    random.seed(seed)\n    shuffled_data = train_data.copy()\n    random.shuffle(shuffled_data)\n    \n    split_idx = int(len(shuffled_data) * (1 - val_ratio))\n    train_split = shuffled_data[:split_idx]\n    val_split = shuffled_data[split_idx:]\n    \n    print(f\"Split train: {len(train_split)} train, {len(val_split)} validation\")\n    return train_split, val_split"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Functions\n",
    "\n",
    "Functions to format data for different model types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def prepare_training_data(data: List[Dict], is_seq2seq: bool = False) -> List[Dict]:\n    \"\"\"Prepare data for fine-tuning.\"\"\"\n    prepared = []\n\n    for item in data:\n        sql_query = item.get(\"query\", {}).get(\"ru\", \"\")\n        question = item.get(\"question\", {}).get(\"ru\", \"\")\n\n        if not sql_query or not question:\n            continue\n\n        sql_query = sql_query.strip()\n        question = question.strip()\n\n        if is_seq2seq:\n            prepared.append({\n                \"input\": f\"SQL: {sql_query}\",\n                \"target\": question\n            })\n        else:\n            formatted_text = f\"SQL: {sql_query}\\nQuestion: {question}\"\n            prepared.append({\"text\": formatted_text})\n\n    print(f\"Prepared {len(prepared)} training examples\")\n    return prepared"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import AutoTokenizer\n\ndef tokenize_function(examples, tokenizer, max_length, is_seq2seq: bool = False):\n    \"\"\"Tokenize the text data.\"\"\"\n    if is_seq2seq:\n        inputs = tokenizer(\n            examples[\"input\"],\n            truncation=True,\n            max_length=max_length,\n            padding=\"max_length\",\n            return_tensors=None,\n        )\n        targets = tokenizer(\n            examples[\"target\"],\n            truncation=True,\n            max_length=max_length,\n            padding=\"max_length\",\n            return_tensors=None,\n        )\n        inputs[\"labels\"] = targets[\"input_ids\"]\n        labels = inputs[\"labels\"]\n        labels = [\n            [(label if label != tokenizer.pad_token_id else -100) for label in labels_seq]\n            for labels_seq in labels\n        ]\n        inputs[\"labels\"] = labels\n        return inputs\n    else:\n        tokenized = tokenizer(\n            examples[\"text\"],\n            truncation=True,\n            max_length=max_length,\n            padding=\"max_length\",\n            return_tensors=None,\n        )\n        return tokenized"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data\n",
    "\n",
    "Load the PAUQ dataset and split according to configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from datasets import Dataset\n\n# Load train data\nprint(\"Loading training data...\")\ntrain_data = load_pauq_data(DATA_DIR, \"train\")\n\n# Load dev data\nprint(\"Loading dev data...\")\ndev_data = load_pauq_data(DATA_DIR, \"dev\")\n\nif USE_DEV_AS_TEST:\n    # Split train into train/val, use dev as test\n    train_split, val_split = split_train_data(train_data, VAL_SPLIT_RATIO)\n    test_data = dev_data\n    print(f\"\\nFinal split: train={len(train_split)}, val={len(val_split)}, test={len(test_data)}\")\nelse:\n    # Use train as train, dev as val (no test)\n    train_split = train_data\n    val_split = dev_data\n    test_data = None\n    print(f\"\\nFinal split: train={len(train_split)}, val={len(val_split)}, test=None\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading Functions\n",
    "\n",
    "Functions to auto-detect and load different model types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\n\ndef load_model_and_tokenizer(model_name: str):\n    \"\"\"Load model and tokenizer. Auto-detects seq2seq vs causal LM.\"\"\"\n    print(f\"Loading model: {model_name}\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    # Try seq2seq first, fallback to causal LM\n    try:\n        from transformers import AutoModelForSeq2SeqLM\n        model = AutoModelForSeq2SeqLM.from_pretrained(\n            model_name,\n            torch_dtype=torch.bfloat16,\n            device_map=\"auto\",\n            low_cpu_mem_usage=True,\n        )\n        is_seq2seq = True\n        print(\"Detected: Seq2Seq model (encoder-decoder)\")\n    except (OSError, ValueError, KeyError):\n        try:\n            from transformers import AutoModelForCausalLM\n            model = AutoModelForCausalLM.from_pretrained(\n                model_name,\n                torch_dtype=torch.bfloat16,\n                device_map=\"auto\",\n                low_cpu_mem_usage=True,\n            )\n            is_seq2seq = False\n            print(\"Detected: Causal LM (decoder-only)\")\n        except Exception as e:\n            raise RuntimeError(f\"Failed to load model {model_name}: {e}\")\n\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n\n    return model, tokenizer, is_seq2seq"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load model and auto-detect type\nmodel, tokenizer, is_seq2seq = load_model_and_tokenizer(MODEL_NAME)\n\nprint(f\"Model loaded on: {model.device}\")\nprint(f\"Model parameters: {model.num_parameters():,}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation Functions\n",
    "\n",
    "Functions to create and tokenize datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def prepare_datasets(tokenizer, max_length, is_seq2seq: bool, \n                      train_split, val_split, test_split=None):\n    \"\"\"Prepare train and validation datasets.\"\"\"\n    # Prepare data\n    train_prepared = prepare_training_data(train_split, is_seq2seq=is_seq2seq)\n    val_prepared = prepare_training_data(val_split, is_seq2seq=is_seq2seq)\n\n    # Create datasets\n    train_dataset = Dataset.from_list(train_prepared)\n    val_dataset = Dataset.from_list(val_prepared)\n\n    # Determine columns to remove\n    cols_to_remove = [\"input\", \"target\"] if is_seq2seq else [\"text\"]\n\n    # Tokenize\n    train_dataset = train_dataset.map(\n        lambda x: tokenize_function(x, tokenizer, max_length, is_seq2seq),\n        batched=True,\n        remove_columns=cols_to_remove,\n    )\n    val_dataset = val_dataset.map(\n        lambda x: tokenize_function(x, tokenizer, max_length, is_seq2seq),\n        batched=True,\n        remove_columns=cols_to_remove,\n    )\n\n    return train_dataset, val_dataset"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prepare datasets\ntrain_dataset, val_dataset = prepare_datasets(\n    tokenizer, MAX_LENGTH, is_seq2seq, train_split, val_split\n)\n\nprint(f\"Train dataset: {len(train_dataset)} samples\")\nprint(f\"Validation dataset: {len(val_dataset)} samples\")\nprint(\"Tokenization complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training\n",
    "\n",
    "Configure the trainer with appropriate settings for the model type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import (\n    Trainer, Seq2SeqTrainer,\n    TrainingArguments, Seq2SeqTrainingArguments,\n    DataCollatorForLanguageModeling, DataCollatorForSeq2Seq\n)\n\n# Data collator and trainer class\nif is_seq2seq:\n    data_collator = DataCollatorForSeq2Seq(\n        tokenizer=tokenizer,\n        model=model,\n        padding=True,\n    )\n    TrainerClass = Seq2SeqTrainer\n    TrainingArgsClass = Seq2SeqTrainingArguments\nelse:\n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer,\n        mlm=False,\n    )\n    TrainerClass = Trainer\n    TrainingArgsClass = TrainingArguments\n\nprint(f\"Using {TrainerClass.__name__}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training arguments\ntraining_args = TrainingArgsClass(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=NUM_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    learning_rate=LEARNING_RATE,\n    warmup_steps=100,\n    logging_steps=10,\n    save_steps=100,\n    eval_steps=100,\n    save_total_limit=3,\n    fp16=False,\n    bf16=torch.cuda.is_bf16_supported(),\n    eval_strategy=\"steps\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    report_to=\"none\",\n    remove_unused_columns=False,\n    predict_with_generate=True if is_seq2seq else False,\n)\n\n# Create trainer\ntrainer = TrainerClass(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator,\n    processing_class=tokenizer,\n)\n\nprint(\"Trainer ready!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "This will take some time depending on your GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Starting training...\")\ntrainer.train()\nprint(\"\\nTraining completed!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(f\"Saving model to {OUTPUT_DIR}\")\ntrainer.save_model(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\nprint(\"Model saved successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation Functions\n",
    "\n",
    "Functions to generate questions from SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_question(model, tokenizer, sql_query: str, is_seq2seq: bool, \n                      max_new_tokens: int = 100, temperature: float = 0.7):\n    \"\"\"Generate a natural language question from a SQL query.\"\"\"\n    if is_seq2seq:\n        prompt = f\"SQL: {sql_query}\"\n    else:\n        prompt = f\"SQL: {sql_query}\\nQuestion:\"\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=max_new_tokens,\n        temperature=temperature,\n        do_sample=True,\n        pad_token_id=tokenizer.pad_token_id,\n    )\n\n    if is_seq2seq:\n        result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    else:\n        full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        result = full_output.split(\"Question:\")[-1].strip()\n\n    return result"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Evaluation\n",
    "\n",
    "Test the model on a few samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Reload model for evaluation (ensures best model is loaded)\neval_model, eval_tokenizer, eval_is_seq2seq = load_model_and_tokenizer(OUTPUT_DIR)\n\nprint(\"\\nSample predictions:\")\nprint(\"=\" * 80)\n\nfor i, item in enumerate(val_split[:5]):\n    sql_query = item.get(\"query\", {}).get(\"ru\", \"\")\n    actual_question = item.get(\"question\", {}).get(\"ru\", \"\")\n\n    predicted_question = generate_question(\n        eval_model, eval_tokenizer, sql_query, eval_is_seq2seq\n    )\n\n    print(f\"\\n--- Sample {i+1} ---\")\n    print(f\"SQL: {sql_query}\")\n    print(f\"Expected: {actual_question}\")\n    print(f\"Predicted: {predicted_question}\")\n    print(\"-\" * 80)"
  },
  {
   "cell_type": "code",
   "source": "val_sqls = [item.get(\"query\", {}).get(\"ru\", \"\") for item in val_split]\nval_references = [item.get(\"question\", {}).get(\"ru\", \"\") for item in val_split]\n\nprint(f\"Generating predictions on {len(val_sqls)} validation samples...\")\nprint(\"=\" * 80)\n\nBATCH_SIZE = 16\nval_hypotheses = generate_questions_batched(\n    eval_model, eval_tokenizer, val_sqls, eval_is_seq2seq,\n    max_new_tokens=100, temperature=0.7, batch_size=BATCH_SIZE\n)\n\nprint(\"\\nComputing metrics...\")\nprint(\"=\" * 80)\nval_metrics = compute_metrics(val_references, val_hypotheses)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"VALIDATION METRICS\")\nprint(\"=\" * 80)\nfor key, value in val_metrics.items():\n    print(f\"{key}: {value:.4f}\")\nprint(\"=\" * 80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "def evaluate_with_test_set(model, tokenizer, test_data, is_seq2seq, num_samples=None):\n    \"\"\"Evaluate model on test set with batched generation.\"\"\"\n    if test_data is None:\n        print(\"No test set available\")\n        return\n\n    if num_samples:\n        test_subset = test_data[:num_samples]\n    else:\n        test_subset = test_data\n\n    print(f\"\\nEvaluating on {len(test_subset)} test samples...\")\n    print(\"=\" * 80)\n\n    test_sqls = [item.get(\"query\", {}).get(\"ru\", \"\") for item in test_subset]\n    test_references = [item.get(\"question\", {}).get(\"ru\", \"\") for item in test_subset]\n\n    BATCH_SIZE = 16\n    test_hypotheses = generate_questions_batched(\n        model, tokenizer, test_sqls, is_seq2seq,\n        max_new_tokens=100, temperature=0.7, batch_size=BATCH_SIZE\n    )\n\n    print(\"\\nComputing metrics...\")\n    print(\"=\" * 80)\n    test_metrics = compute_metrics(test_references, test_hypotheses)\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"TEST METRICS\")\n    print(\"=\" * 80)\n    for key, value in test_metrics.items():\n        print(f\"{key}: {value:.4f}\")\n    print(\"=\" * 80)\n\n    print(\"\\nSample predictions (first 5):\")\n    print(\"-\" * 80)\n    for i in range(min(5, len(test_subset))):\n        sql_query = test_subset[i].get(\"query\", {}).get(\"ru\", \"\")\n        actual_question = test_subset[i].get(\"question\", {}).get(\"ru\", \"\")\n        predicted_question = test_hypotheses[i]\n        print(f\"\\n--- Sample {i+1} ---\")\n        print(f\"SQL: {sql_query}\")\n        print(f\"Expected: {actual_question}\")\n        print(f\"Predicted: {predicted_question}\")\n\n    print(\"\\nTest evaluation complete!\")",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import numpy as np\nfrom sentence_transformers import SentenceTransformer, util\n\n# LaBSE Model Caching\nlabse_model = None\n\ndef get_labse_model():\n    global labse_model\n    if labse_model is None:\n        print(\"Loading LaBSE model...\")\n        labse_model = SentenceTransformer('sentence-transformers/LaBSE')\n    return labse_model\n\ndef generate_questions_batched(model, tokenizer, sql_queries, is_seq2seq,\n                                   max_new_tokens=100, temperature=0.7, batch_size=16):\n    \"\"\"Generate questions from SQL queries in batches.\"\"\"\n    all_results = []\n    num_batches = (len(sql_queries) + batch_size - 1) // batch_size\n\n    for batch_idx in range(num_batches):\n        start_idx = batch_idx * batch_size\n        end_idx = min(start_idx + batch_size, len(sql_queries))\n        batch_sqls = sql_queries[start_idx:end_idx]\n\n        if is_seq2seq:\n            prompts = [f\"SQL: {sql}\" for sql in batch_sqls]\n            inputs = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n        else:\n            prompts = [f\"SQL: {sql}\\nQuestion:\" for sql in batch_sqls]\n            inputs = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\n\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            do_sample=True,\n            pad_token_id=tokenizer.pad_token_id,\n        )\n\n        batch_results = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n        if not is_seq2seq:\n            batch_results = [output.split(\"Question:\")[-1].strip() for output in batch_results]\n\n        all_results.extend(batch_results)\n        print(f\"Processed batch {batch_idx + 1}/{num_batches}\")\n\n    return all_results\n\ndef compute_metrics(references, hypotheses):\n    from sacrebleu.metrics import BLEU, CHRF\n    import torch\n    from rouge_score import rouge_scorer\n\n    print(\"Computing BLEU...\")\n    bleu_metric = BLEU()\n    bleu_result = bleu_metric.corpus_score(hypotheses, [references])\n\n    print(\"Computing ROUGE...\")\n    rouge_scorer_instance = rouge_scorer.RougeScorer(\n        ['rouge1', 'rouge2', 'rougeL'], use_stemmer=True\n    )\n    rouge1_scores = []\n    rouge2_scores = []\n    rougeL_scores = []\n    for ref, hyp in zip(references, hypotheses):\n        scores = rouge_scorer_instance.score(ref, hyp)\n        rouge1_scores.append(scores['rouge1'].fmeasure)\n        rouge2_scores.append(scores['rouge2'].fmeasure)\n        rougeL_scores.append(scores['rougeL'].fmeasure)\n\n    print(\"Computing CHRF...\")\n    chrf_metric = CHRF()\n    chrf_result = chrf_metric.corpus_score(hypotheses, [references])\n\n    labse = get_labse_model()\n    ref_embeddings = labse.encode(references, convert_to_tensor=True)\n    hyp_embeddings = labse.encode(hypotheses, convert_to_tensor=True)\n    similarities = util.cos_sim(hyp_embeddings, ref_embeddings)\n    similarity_scores = torch.diagonal(similarities).cpu().numpy()\n\n    return {\n        'BLEU': bleu_result.score,\n        'ROUGE-1': np.mean(rouge1_scores) * 100,\n        'ROUGE-2': np.mean(rouge2_scores) * 100,\n        'ROUGE-L': np.mean(rougeL_scores) * 100,\n        'CHRF': chrf_result.score,\n        'LaBSE-Similarity': np.mean(similarity_scores) * 100,\n    }",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Metrics and Batched Generation\n\nFunctions for batched generation and computing evaluation metrics:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "val_sqls = [item.get(\"query\", {}).get(\"ru\", \"\") for item in val_split]\nval_references = [item.get(\"question\", {}).get(\"ru\", \"\") for item in val_split]\n\nprint(f\"Generating predictions on {len(val_sqls)} validation samples...\")\nprint(\"=\" * 80)\n\nBATCH_SIZE = 16\nval_hypotheses = generate_questions_batched(\n    eval_model, eval_tokenizer, val_sqls, eval_is_seq2seq,\n    max_new_tokens=100, temperature=0.7, batch_size=BATCH_SIZE\n)\n\nprint(\"\\nComputing metrics...\")\nprint(\"=\" * 80)\nval_metrics = compute_metrics(val_references, val_hypotheses)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"VALIDATION METRICS\")\nprint(\"=\" * 80)\nfor key, value in val_metrics.items():\n    print(f\"{key}: {value:.4f}\")\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Metrics\n",
    "\n",
    "Functions to compute BLEU, ROUGE, CHRF, and LaBSE similarity scores:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set Evaluation\n",
    "\n",
    "Evaluate on the test set (dev data if USE_DEV_AS_TEST=True):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def evaluate_with_test_set(model, tokenizer, test_data, is_seq2seq, num_samples=None):\n    \"\"\"Evaluate model on test set with batched generation.\"\"\"\n    if test_data is None:\n        print(\"No test set available\")\n        return\n\n    if num_samples:\n        test_subset = test_data[:num_samples]\n    else:\n        test_subset = test_data\n\n    print(f\"\\nEvaluating on {len(test_subset)} test samples...\")\n    print(\"=\" * 80)\n\n    test_sqls = [item.get(\"query\", {}).get(\"ru\", \"\") for item in test_subset]\n    test_references = [item.get(\"question\", {}).get(\"ru\", \"\") for item in test_subset]\n\n    BATCH_SIZE = 16\n    test_hypotheses = generate_questions_batched(\n        model, tokenizer, test_sqls, is_seq2seq,\n        max_new_tokens=100, temperature=0.7, batch_size=BATCH_SIZE\n    )\n\n    print(\"\\nComputing metrics...\")\n    print(\"=\" * 80)\n    test_metrics = compute_metrics(test_references, test_hypotheses)\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"TEST METRICS\")\n    print(\"=\" * 80)\n    for key, value in test_metrics.items():\n        print(f\"{key}: {value:.4f}\")\n    print(\"=\" * 80)\n\n    print(\"\\nSample predictions (first 5):\")\n    print(\"-\" * 80)\n    for i in range(min(5, len(test_subset))):\n        sql_query = test_subset[i].get(\"query\", {}).get(\"ru\", \"\")\n        actual_question = test_subset[i].get(\"question\", {}).get(\"ru\", \"\")\n        predicted_question = test_hypotheses[i]\n        print(f\"\\n--- Sample {i+1} ---\")\n        print(f\"SQL: {sql_query}\")\n        print(f\"Expected: {actual_question}\")\n        print(f\"Predicted: {predicted_question}\")\n\n    print(\"\\nTest evaluation complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if test_data:\n    evaluate_with_test_set(eval_model, eval_tokenizer, test_data, eval_is_seq2seq, num_samples=10)\nelse:\n    print(\"No test set configured (USE_DEV_AS_TEST=False)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom SQL Test\n",
    "\n",
    "Test with your own SQL queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test with custom SQL\ntest_sql = \"SELECT name, age FROM users WHERE age > 25 ORDER BY name;\"\n\npredicted = generate_question(eval_model, eval_tokenizer, test_sql, eval_is_seq2seq)\n\nprint(f\"SQL: {test_sql}\")\nprint(f\"Generated Question: {predicted}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Model (Optional)\n",
    "\n",
    "Download the trained model to your local machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import shutil\n\nzip_filename = \"sql_to_text_model.zip\"\nprint(f\"Zipping model to {zip_filename}...\")\nshutil.make_archive(zip_filename.replace('.zip', ''), 'zip', OUTPUT_DIR)\nprint(\"Zipping complete!\")\n\nprint(\"\\nStarting download...\")\nfiles.download(zip_filename)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}