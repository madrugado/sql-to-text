{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SQL-to-Text Training Notebook\n",
        "\n",
        "This notebook fine-tunes models (both seq2seq and causal LMs) to generate natural language questions from SQL queries using the PAUQ dataset.\n",
        "\n",
        "## Features\n",
        "- Model-agnostic: Supports T5, RuT5, BART, Qwen, Llama, and more\n",
        "- Auto-detects model architecture (seq2seq vs causal LM)\n",
        "- Train/validation/test split functionality\n",
        "- Evaluation with multiple metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q torch transformers datasets accelerate sacremoses sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Set the model and training parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration - Change to any HuggingFace model\n",
        "# Seq2Seq models: cointegrated/rut5-base, google/flan-t5-base, facebook/bart-base\n",
        "# Causal LMs: Qwen/Qwen2.5-0.5B-Instruct, meta-llama/Llama-3.2-1B-Instruct\n",
        "MODEL_NAME = \"cointegrated/rut5-base\"\n",
        "\n",
        "# Paths - using VM file system\n",
        "DATA_DIR = \".\"  # Current directory where data files were uploaded\n",
        "OUTPUT_DIR = \"./sql_to_text_model\"\n",
        "\n",
        "# Data splitting mode\n",
        "# True: Split train into train/val, use dev as test\n",
        "# False: Use train as train, dev as val (no test set)\n",
        "USE_DEV_AS_TEST = True\n",
        "VAL_SPLIT_RATIO = 0.2  # 20% of train becomes validation\n",
        "\n",
        "# Training configuration\n",
        "MAX_LENGTH = 512\n",
        "BATCH_SIZE = 4\n",
        "GRADIENT_ACCUMULATION_STEPS = 4\n",
        "NUM_EPOCHS = 3\n",
        "LEARNING_RATE = 2e-4\n",
        "\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
        "print(f\"Use dev as test: {USE_DEV_AS_TEST}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upload Data\n",
        "\n",
        "Upload your `pauq_train.json` and `pauq_dev.json` files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "print(\"Please upload pauq_train.json and pauq_dev.json\")\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading Functions\n",
        "\n",
        "Functions to load and split the PAUQ dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from typing import List, Dict, Tuple\n",
        "import random\n",
        "\n",
        "def load_pauq_data(data_dir: str, split: str = \"train\") -> List[Dict]:\n",
        "    \"\"\"Load PAUQ dataset from JSON file.\"\"\"\n",
        "    filename = f\"pauq_{split}.json\"\n",
        "    filepath = os.path.join(data_dir, filename)\n",
        "\n",
        "    if not os.path.exists(filepath):\n",
        "        raise FileNotFoundError(f\"Data file not found: {filepath}\")\n",
        "\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    print(f\"Loaded {len(data)} examples from {filename}\")\n",
        "    return data\n",
        "\n",
        "def split_train_data(train_data: List[Dict], val_ratio: float = 0.2, seed: int = 42) -> Tuple[List[Dict], List[Dict]]:\n",
        "    \"\"\"Split training data into train and validation sets.\"\"\"\n",
        "    random.seed(seed)\n",
        "    shuffled_data = train_data.copy()\n",
        "    random.shuffle(shuffled_data)\n",
        "    \n",
        "    split_idx = int(len(shuffled_data) * (1 - val_ratio))\n",
        "    train_split = shuffled_data[:split_idx]\n",
        "    val_split = shuffled_data[split_idx:]\n",
        "    \n",
        "    print(f\"Split train: {len(train_split)} train, {len(val_split)} validation\")\n",
        "    return train_split, val_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preparation Functions\n",
        "\n",
        "Functions to format data for different model types:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_training_data(data: List[Dict], is_seq2seq: bool = False) -> List[Dict]:\n",
        "    \"\"\"Prepare data for fine-tuning.\"\"\"\n",
        "    prepared = []\n",
        "\n",
        "    for item in data:\n",
        "        sql_query = item.get(\"query\", {}).get(\"en\", \"\")\n",
        "        question = item.get(\"question\", {}).get(\"en\", \"\")\n",
        "\n",
        "        if not sql_query or not question:\n",
        "            continue\n",
        "\n",
        "        sql_query = sql_query.strip()\n",
        "        question = question.strip()\n",
        "\n",
        "        if is_seq2seq:\n",
        "            prepared.append({\n",
        "                \"input\": f\"SQL: {sql_query}\",\n",
        "                \"target\": question\n",
        "            })\n",
        "        else:\n",
        "            formatted_text = f\"SQL: {sql_query}\\nQuestion: {question}\"\n",
        "            prepared.append({\"text\": formatted_text})\n",
        "\n",
        "    print(f\"Prepared {len(prepared)} training examples\")\n",
        "    return prepared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "def tokenize_function(examples, tokenizer, max_length, is_seq2seq: bool = False):\n",
        "    \"\"\"Tokenize the text data.\"\"\"\n",
        "    if is_seq2seq:\n",
        "        inputs = tokenizer(\n",
        "            examples[\"input\"],\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=None,\n",
        "        )\n",
        "        targets = tokenizer(\n",
        "            examples[\"target\"],\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=None,\n",
        "        )\n",
        "        inputs[\"labels\"] = targets[\"input_ids\"]\n",
        "        labels = inputs[\"labels\"]\n",
        "        labels = [\n",
        "            [(label if label != tokenizer.pad_token_id else -100) for label in labels_seq]\n",
        "            for labels_seq in labels\n",
        "        ]\n",
        "        inputs[\"labels\"] = labels\n",
        "        return inputs\n",
        "    else:\n",
        "        tokenized = tokenizer(\n",
        "            examples[\"text\"],\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=None,\n",
        "        )\n",
        "        return tokenized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Prepare Data\n",
        "\n",
        "Load the PAUQ dataset and split according to configuration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Load train data\n",
        "print(\"Loading training data...\")\n",
        "train_data = load_pauq_data(DATA_DIR, \"train\")\n",
        "\n",
        "# Load dev data\n",
        "print(\"Loading dev data...\")\n",
        "dev_data = load_pauq_data(DATA_DIR, \"dev\")\n",
        "\n",
        "if USE_DEV_AS_TEST:\n",
        "    # Split train into train/val, use dev as test\n",
        "    train_split, val_split = split_train_data(train_data, VAL_SPLIT_RATIO)\n",
        "    test_data = dev_data\n",
        "    print(f\"\\nFinal split: train={len(train_split)}, val={len(val_split)}, test={len(test_data)}\")\n",
        "else:\n",
        "    # Use train as train, dev as val (no test)\n",
        "    train_split = train_data\n",
        "    val_split = dev_data\n",
        "    test_data = None\n",
        "    print(f\"\\nFinal split: train={len(train_split)}, val={len(val_split)}, test=None\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Loading Functions\n",
        "\n",
        "Functions to auto-detect and load different model types:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def load_model_and_tokenizer(model_name: str):\n",
        "    \"\"\"Load model and tokenizer. Auto-detects seq2seq vs causal LM.\"\"\"\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Try seq2seq first, fallback to causal LM\n",
        "    try:\n",
        "        from transformers import AutoModelForSeq2SeqLM\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=\"auto\",\n",
        "            low_cpu_mem_usage=True,\n",
        "        )\n",
        "        is_seq2seq = True\n",
        "        print(\"Detected: Seq2Seq model (encoder-decoder)\")\n",
        "    except (OSError, ValueError, KeyError):\n",
        "        try:\n",
        "            from transformers import AutoModelForCausalLM\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                torch_dtype=torch.bfloat16,\n",
        "                device_map=\"auto\",\n",
        "                low_cpu_mem_usage=True,\n",
        "            )\n",
        "            is_seq2seq = False\n",
        "            print(\"Detected: Causal LM (decoder-only)\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to load model {model_name}: {e}\")\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    return model, tokenizer, is_seq2seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model and auto-detect type\n",
        "model, tokenizer, is_seq2seq = load_model_and_tokenizer(MODEL_NAME)\n",
        "\n",
        "print(f\"Model loaded on: {model.device}\")\n",
        "print(f\"Model parameters: {model.num_parameters():,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Preparation Functions\n",
        "\n",
        "Functions to create and tokenize datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_datasets(tokenizer, max_length, is_seq2seq: bool, \n",
        "                      train_split, val_split, test_split=None):\n",
        "    \"\"\"Prepare train and validation datasets.\"\"\"\n",
        "    # Prepare data\n",
        "    train_prepared = prepare_training_data(train_split, is_seq2seq=is_seq2seq)\n",
        "    val_prepared = prepare_training_data(val_split, is_seq2seq=is_seq2seq)\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = Dataset.from_list(train_prepared)\n",
        "    val_dataset = Dataset.from_list(val_prepared)\n",
        "\n",
        "    # Determine columns to remove\n",
        "    cols_to_remove = [\"input\", \"target\"] if is_seq2seq else [\"text\"]\n",
        "\n",
        "    # Tokenize\n",
        "    train_dataset = train_dataset.map(\n",
        "        lambda x: tokenize_function(x, tokenizer, max_length, is_seq2seq),\n",
        "        batched=True,\n",
        "        remove_columns=cols_to_remove,\n",
        "    )\n",
        "    val_dataset = val_dataset.map(\n",
        "        lambda x: tokenize_function(x, tokenizer, max_length, is_seq2seq),\n",
        "        batched=True,\n",
        "        remove_columns=cols_to_remove,\n",
        "    )\n",
        "\n",
        "    return train_dataset, val_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare datasets\n",
        "train_dataset, val_dataset = prepare_datasets(\n",
        "    tokenizer, MAX_LENGTH, is_seq2seq, train_split, val_split\n",
        ")\n",
        "\n",
        "print(f\"Train dataset: {len(train_dataset)} samples\")\n",
        "print(f\"Validation dataset: {len(val_dataset)} samples\")\n",
        "print(\"Tokenization complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Training\n",
        "\n",
        "Configure the trainer with appropriate settings for the model type:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    Trainer, Seq2SeqTrainer,\n",
        "    TrainingArguments, Seq2SeqTrainingArguments,\n",
        "    DataCollatorForLanguageModeling, DataCollatorForSeq2Seq\n",
        ")\n",
        "\n",
        "# Data collator and trainer class\n",
        "if is_seq2seq:\n",
        "    data_collator = DataCollatorForSeq2Seq(\n",
        "        tokenizer=tokenizer,\n",
        "        model=model,\n",
        "        padding=True,\n",
        "    )\n",
        "    TrainerClass = Seq2SeqTrainer\n",
        "    TrainingArgsClass = Seq2SeqTrainingArguments\n",
        "else:\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False,\n",
        "    )\n",
        "    TrainerClass = Trainer\n",
        "    TrainingArgsClass = TrainingArguments\n",
        "\n",
        "print(f\"Using {TrainerClass.__name__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArgsClass(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    warmup_steps=100,\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        "    eval_steps=100,\n",
        "    save_total_limit=3,\n",
        "    fp16=False,\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    eval_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False,\n",
        "    predict_with_generate=True if is_seq2seq else False,\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "trainer = TrainerClass(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"Trainer ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the Model\n",
        "\n",
        "This will take some time depending on your GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "print(\"\\nTraining completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Saving model to {OUTPUT_DIR}\")\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(\"Model saved successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generation Functions\n",
        "\n",
        "Functions to generate questions from SQL:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_question(model, tokenizer, sql_query: str, is_seq2seq: bool, \n",
        "                      max_new_tokens: int = 100, temperature: float = 0.7):\n",
        "    \"\"\"Generate a natural language question from a SQL query.\"\"\"\n",
        "    if is_seq2seq:\n",
        "        prompt = f\"SQL: {sql_query}\"\n",
        "    else:\n",
        "        prompt = f\"SQL: {sql_query}\\nQuestion:\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=temperature,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )\n",
        "\n",
        "    if is_seq2seq:\n",
        "        result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    else:\n",
        "        full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        result = full_output.split(\"Question:\")[-1].strip()\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick Evaluation\n",
        "\n",
        "Test the model on a few samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reload model for evaluation (ensures best model is loaded)\n",
        "eval_model, eval_tokenizer, eval_is_seq2seq = load_model_and_tokenizer(OUTPUT_DIR)\n",
        "\n",
        "print(\"\\nSample predictions:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, item in enumerate(val_split[:5]):\n",
        "    sql_query = item.get(\"query\", {}).get(\"en\", \"\")\n",
        "    actual_question = item.get(\"question\", {}).get(\"en\", \"\")\n",
        "\n",
        "    predicted_question = generate_question(\n",
        "        eval_model, eval_tokenizer, sql_query, eval_is_seq2seq\n",
        "    )\n",
        "\n",
        "    print(f\"\\n--- Sample {i+1} ---\")\n",
        "    print(f\"SQL: {sql_query}\")\n",
        "    print(f\"Expected: {actual_question}\")\n",
        "    print(f\"Predicted: {predicted_question}\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Set Evaluation\n",
        "\n",
        "Evaluate on the test set (dev data if USE_DEV_AS_TEST=True):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_with_test_set(model, tokenizer, test_data: List[Dict], \n",
        "                             is_seq2seq: bool, num_samples: int = None):\n",
        "    \"\"\"Evaluate model on test set.\"\"\"\n",
        "    if test_data is None:\n",
        "        print(\"No test set available\")\n",
        "        return\n",
        "\n",
        "    if num_samples:\n",
        "        test_subset = test_data[:num_samples]\n",
        "    else:\n",
        "        test_subset = test_data\n",
        "\n",
        "    print(f\"\\nEvaluating on {len(test_subset)} test samples...\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    for i, item in enumerate(test_subset[:5]):\n",
        "        sql_query = item.get(\"query\", {}).get(\"en\", \"\")\n",
        "        actual_question = item.get(\"question\", {}).get(\"en\", \"\")\n",
        "\n",
        "        predicted_question = generate_question(\n",
        "            model, tokenizer, sql_query, is_seq2seq\n",
        "        )\n",
        "\n",
        "        print(f\"\\n--- Test Sample {i+1} ---\")\n",
        "        print(f\"SQL: {sql_query}\")\n",
        "        print(f\"Expected: {actual_question}\")\n",
        "        print(f\"Predicted: {predicted_question}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "    print(\"\\nTest evaluation complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if test_data:\n",
        "    evaluate_with_test_set(eval_model, eval_tokenizer, test_data, eval_is_seq2seq, num_samples=10)\n",
        "else:\n",
        "    print(\"No test set configured (USE_DEV_AS_TEST=False)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom SQL Test\n",
        "\n",
        "Test with your own SQL queries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with custom SQL\n",
        "test_sql = \"SELECT name, age FROM users WHERE age > 25 ORDER BY name;\"\n",
        "\n",
        "predicted = generate_question(eval_model, eval_tokenizer, test_sql, eval_is_seq2seq)\n",
        "\n",
        "print(f\"SQL: {test_sql}\")\n",
        "print(f\"Generated Question: {predicted}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Model (Optional)\n",
        "\n",
        "Download the trained model to your local machine:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "zip_filename = \"sql_to_text_model.zip\"\n",
        "print(f\"Zipping model to {zip_filename}...\")\n",
        "shutil.make_archive(zip_filename.replace('.zip', ''), 'zip', OUTPUT_DIR)\n",
        "print(\"Zipping complete!\")\n",
        "\n",
        "print(\"\\nStarting download...\")\n",
        "files.download(zip_filename)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
