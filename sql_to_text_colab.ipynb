{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SQL-to-Text Training and Evaluation (Colab)\n",
        "\n",
        "This notebook fine-tunes and evaluates a model for SQL-to-text generation using PAUQ dataset.\n",
        "\n",
        "## Supported Model Types\n",
        "- **Seq2Seq models**: RuT5, T5, BART, mBART, etc.\n",
        "- **Causal LMs**: Qwen, Llama, Mistral, etc.\n",
        "\n",
        "The code automatically detects model type and uses the appropriate training/inference approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Install required dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q torch transformers datasets accelerate peft rouge-score nltk sacrebleu sentencepiece tqdm\n",
        "\n",
        "print(\"Dependencies installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mount Google Drive (Optional)\n",
        "\n",
        "Mount Google Drive to save your trained model and results permanently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set your Google Drive path\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/sql_to_text\"\n",
        "OUTPUT_DIR = f\"{DRIVE_PATH}/model_output\"\n",
        "DATA_DIR = f\"{DRIVE_PATH}/data\"\n",
        "\n",
        "# Create directories\n",
        "!mkdir -p {OUTPUT_DIR}\n",
        "!mkdir -p {DATA_DIR}\n",
        "\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"Data directory: {DATA_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upload Data Files\n",
        "\n",
        "Upload your PAUQ dataset files (`pauq_train.json` and `pauq_dev.json`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "print(\"Please upload pauq_train.json and pauq_dev.json\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Move uploaded files to data directory\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith('.json'):\n",
        "        shutil.move(filename, os.path.join(DATA_DIR, filename))\n",
        "        print(f\"Moved {filename} to {DATA_DIR}\")\n",
        "\n",
        "# Verify files\n",
        "train_file = os.path.join(DATA_DIR, \"pauq_train.json\")\n",
        "dev_file = os.path.join(DATA_DIR, \"pauq_dev.json\")\n",
        "\n",
        "if os.path.exists(train_file) and os.path.exists(dev_file):\n",
        "    print(\"\\nData files ready!\")\n",
        "else:\n",
        "    print(\"\\nWarning: Missing data files!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Libraries and Define Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n# ===================== IMPORTS =====================\nimport json\nimport os\nimport csv\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional\n\nimport torch\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    AutoModelForSeq2SeqLM,\n    TrainingArguments,\n    Seq2SeqTrainingArguments,\n    Trainer,\n    Seq2SeqTrainer,\n    DataCollatorForLanguageModeling,\n    DataCollatorForSeq2Seq,\n)\nfrom rouge_score import rouge_scorer\nimport nltk\nfrom nltk.translate.bleu_score import SmoothingFunction, corpus_bleu\nfrom tqdm import tqdm\nfrom sacrebleu.metrics import CHRF\n\n# ===================== DATA FUNCTIONS =====================\ndef load_pauq_data(data_dir: str, split: str = \"train\") -> List[Dict]:\n    \"\"\"Load PAUQ dataset from JSON file.\"\"\"\n    filename = f\"pauq_{split}.json\"\n    filepath = os.path.join(data_dir, filename)\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f\"Data file not found: {filepath}\")\n    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n    print(f\"Loaded {len(data)} examples from {filename}\")\n    return data\n\ndef prepare_training_data(data: List[Dict], is_seq2seq: bool = False) -> List[Dict]:\n    \"\"\"Prepare data for fine-tuning.\"\"\"\n    prepared = []\n    for item in data:\n        sql_query = item.get(\"query\", {}).get(\"en\", \"\")\n        question = item.get(\"question\", {}).get(\"en\", \"\")\n        if not sql_query or not question:\n            continue\n        sql_query = sql_query.strip()\n        question = question.strip()\n        if is_seq2seq:\n            prepared.append({\n                \"input\": f\"SQL: {sql_query}\",\n                \"target\": question\n            })\n        else:\n            formatted_text = f\"SQL: {sql_query}\\\\nQuestion: {question}\"\n            prepared.append({\"text\": formatted_text})\n    print(f\"Prepared {len(prepared)} training examples\")\n    return prepared\n\ndef tokenize_function(examples, tokenizer, max_length, is_seq2seq: bool = False):\n    \"\"\"Tokenize text data.\"\"\"\n    if is_seq2seq:\n        inputs = tokenizer(\n            examples[\"input\"],\n            truncation=True,\n            max_length=max_length,\n            padding=\"max_length\",\n            return_tensors=None,\n        )\n        targets = tokenizer(\n            examples[\"target\"],\n            truncation=True,\n            max_length=max_length,\n            padding=\"max_length\",\n            return_tensors=None,\n        )\n        inputs[\"labels\"] = targets[\"input_ids\"]\n        labels = inputs[\"labels\"]\n        labels = [\n            [(label if label != tokenizer.pad_token_id else -100) for label in labels_seq]\n            for labels_seq in labels\n        ]\n        inputs[\"labels\"] = labels\n        return inputs\n    else:\n        tokenized = tokenizer(\n            examples[\"text\"],\n            truncation=True,\n            max_length=max_length,\n            padding=\"max_length\",\n            return_tensors=None,\n        )\n        return tokenized\n\n# ===================== MODEL LOADING =====================\ndef load_model_and_tokenizer(model_name: str):\n    \"\"\"Load model and tokenizer. Returns model, tokenizer, and is_seq2seq flag.\"\"\"\n    print(f\"Loading model: {model_name}\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    try:\n        model = AutoModelForSeq2SeqLM.from_pretrained(\n            model_name,\n            torch_dtype=torch.bfloat16,\n            device_map=\"auto\",\n            low_cpu_mem_usage=True,\n        )\n        is_seq2seq = True\n        print(\"Detected: Seq2Seq model (encoder-decoder architecture)\")\n    except (OSError, ValueError, KeyError):\n        try:\n            model = AutoModelForCausalLM.from_pretrained(\n                model_name,\n                torch_dtype=torch.bfloat16,\n                device_map=\"auto\",\n                low_cpu_mem_usage=True,\n            )\n            is_seq2seq = False\n            print(\"Detected: Causal LM (decoder-only architecture)\")\n        except Exception as e:\n            raise RuntimeError(f\"Failed to load model {model_name}: {e}\")\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    return model, tokenizer, is_seq2seq\n\ndef prepare_datasets(tokenizer, config, is_seq2seq: bool = False):\n    \"\"\"Prepare train and validation datasets.\"\"\"\n    print(\"Loading training data...\")\n    train_data = load_pauq_data(config.data_dir, \"train\")\n    train_prepared = prepare_training_data(train_data, is_seq2seq=is_seq2seq)\n    print(\"Loading validation data...\")\n    val_data = load_pauq_data(config.data_dir, \"dev\")\n    val_prepared = prepare_training_data(val_data, is_seq2seq=is_seq2seq)\n    train_dataset = Dataset.from_list(train_prepared)\n    val_dataset = Dataset.from_list(val_prepared)\n    cols_to_remove = [\"input\", \"target\"] if is_seq2seq else [\"text\"]\n    train_dataset = train_dataset.map(\n        lambda x: tokenize_function(x, tokenizer, config.max_length, is_seq2seq=is_seq2seq),\n        batched=True,\n        remove_columns=cols_to_remove,\n    )\n    val_dataset = val_dataset.map(\n        lambda x: tokenize_function(x, tokenizer, config.max_length, is_seq2seq=is_seq2seq),\n        batched=True,\n        remove_columns=cols_to_remove,\n    )\n    return train_dataset, val_dataset\n\n# ===================== TRAINING =====================\n@dataclass\nclass TrainingConfig:\n    \"\"\"Training hyperparameters.\"\"\"\n    model_name: str = \"cointegrated/rut5-base\"\n    output_dir: str = \"./output/sql_to_text\"\n    data_dir: str = \"./data\"\n    max_length: int = 512\n    batch_size: int = 4\n    gradient_accumulation_steps: int = 4\n    num_epochs: int = 3\n    learning_rate: float = 2e-4\n    warmup_steps: int = 100\n    logging_steps: int = 10\n    save_steps: int = 100\n    eval_steps: int = 100\n\ndef train_model(config: TrainingConfig):\n    \"\"\"Main training function.\"\"\"\n    print(\"Starting SQL-to-text training...\")\n    model, tokenizer, is_seq2seq = load_model_and_tokenizer(config.model_name)\n    train_dataset, val_dataset = prepare_datasets(tokenizer, config, is_seq2seq=is_seq2seq)\n    if is_seq2seq:\n        data_collator = DataCollatorForSeq2Seq(\n            tokenizer=tokenizer,\n            model=model,\n            padding=True,\n        )\n        TrainerClass = Seq2SeqTrainer\n        TrainingArgsClass = Seq2SeqTrainingArguments\n    else:\n        data_collator = DataCollatorForLanguageModeling(\n            tokenizer=tokenizer,\n            mlm=False,\n        )\n        TrainerClass = Trainer\n        TrainingArgsClass = TrainingArguments\n    training_args = TrainingArgsClass(\n        output_dir=config.output_dir,\n        num_train_epochs=config.num_epochs,\n        per_device_train_batch_size=config.batch_size,\n        per_device_eval_batch_size=config.batch_size,\n        gradient_accumulation_steps=config.gradient_accumulation_steps,\n        learning_rate=config.learning_rate,\n        warmup_steps=config.warmup_steps,\n        logging_steps=config.logging_steps,\n        save_steps=config.save_steps,\n        eval_steps=config.eval_steps,\n        save_total_limit=3,\n        fp16=False,\n        bf16=torch.cuda.is_bf16_supported(),\n        eval_strategy=\"steps\",\n        load_best_model_at_end=True,\n        metric_for_best_model=\"eval_loss\",\n        greater_is_better=False,\n        report_to=\"none\",\n        remove_unused_columns=False,\n        predict_with_generate=True if is_seq2seq else False,\n    )\n    trainer = TrainerClass(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        data_collator=data_collator,\n        processing_class=tokenizer,\n    )\n    print(\"Starting training...\")\n    trainer.train()\n    print(f\"Saving final model to {config.output_dir}\")\n    trainer.save_model(config.output_dir)\n    tokenizer.save_pretrained(config.output_dir)\n    print(\"Training completed!\")\n    return trainer\n\n# ===================== EVALUATION =====================\ndef download_nltk_data():\n    \"\"\"Download required NLTK data.\"\"\"\n    try:\n        nltk.data.find(\"tokenizers/punkt_tab\")\n    except LookupError:\n        nltk.download(\"punkt_tab\", quiet=True)\n\ndef generate_question(model, tokenizer, sql_query: str, max_new_tokens: int = 100, temperature: float = 0.7, do_sample: bool = True) -> str:\n    \"\"\"Generate a question from SQL query.\"\"\"\n    examples = \"\"\"Generate a natural language question from this SQL query.\n\nSQL: SELECT count(*) FROM singer;\nQuestion: How many singers do we have?\n\nSQL: SELECT name, country, age FROM singer ORDER BY age DESC;\nQuestion: Show the name, country and age of all singers, ordered by age from oldest to youngest.\n\nSQL: SELECT name FROM singer WHERE country = 'USA';\nQuestion: What are the names of singers from USA?\n\nSQL: SELECT count(*) FROM album WHERE singer_id = 1;\nQuestion: How many albums does singer 1 have?\n\nSQL: {sql_query}\nQuestion:\"\"\"\n    prompt = examples.format(sql_query=sql_query)\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    with torch.no_grad():\n        if getattr(model, \"model_type\", \"causal\") == \"seq2seq\":\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=max_new_tokens,\n                temperature=temperature if do_sample else None,\n                do_sample=do_sample,\n                pad_token_id=tokenizer.pad_token_id,\n            )\n            predicted_question = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        else:\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=max_new_tokens,\n                temperature=temperature,\n                do_sample=do_sample,\n                pad_token_id=tokenizer.pad_token_id,\n            )\n            full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n            predicted_question = full_output.split(\"Question:\")[-1].strip()\n    predicted_question = predicted_question.split(\"\\\\n\")[0].strip()\n    return predicted_question\n\ndef compute_bleu_score(references: List[str], hypotheses: List[str]) -> Dict[str, float]:\n    \"\"\"Compute BLEU score.\"\"\"\n    smoothing = SmoothingFunction()\n    ref_tokens = [nltk.word_tokenize(ref.lower()) for ref in references]\n    hyp_tokens = [nltk.word_tokenize(hyp.lower()) for hyp in hypotheses]\n    bleu_scores = {}\n    for n in range(1, 5):\n        weights = tuple([1.0/n] * n + [0.0] * (4-n))\n        try:\n            score = corpus_bleu(\n                [[ref] for ref in ref_tokens],\n                hyp_tokens,\n                weights=weights,\n                smoothing_function=smoothing.method1,\n            )\n            bleu_scores[f\"BLEU-{n}\"] = score * 100\n        except Exception:\n            bleu_scores[f\"BLEU-{n}\"] = 0.0\n    return bleu_scores\n\ndef compute_chrf_score(references: List[str], hypotheses: List[str]) -> Dict[str, float]:\n    \"\"\"Compute CHRF score.\"\"\"\n    chrf = CHRF()\n    ref_str = \" ||| \".join(refs for refs in references)\n    hyp_str = \"\\\\n\".join(hypotheses)\n    result = chrf.corpus_score(hyp_str, [ref_str])\n    return {\"CHRF\": result.score}\n\ndef compute_rouge_score(references: List[str], hypotheses: List[str]) -> Dict[str, float]:\n    \"\"\"Compute ROUGE scores.\"\"\"\n    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n    rouge1_scores = []\n    rouge2_scores = []\n    rougeL_scores = []\n    for ref, hyp in zip(references, hypotheses):\n        scores = scorer.score(ref, hyp)\n        rouge1_scores.append(scores[\"rouge1\"].fmeasure)\n        rouge2_scores.append(scores[\"rouge2\"].fmeasure)\n        rougeL_scores.append(scores[\"rougeL\"].fmeasure)\n    return {\n        \"ROUGE-1\": sum(rouge1_scores) / len(rouge1_scores) * 100,\n        \"ROUGE-2\": sum(rouge2_scores) / len(rouge2_scores) * 100,\n        \"ROUGE-L\": sum(rougeL_scores) / len(rougeL_scores) * 100,\n    }\n\ndef evaluate_model(model_path: str, num_samples: Optional[int] = None, output_file: Optional[str] = None) -> Dict:\n    \"\"\"Evaluate model on dev set.\"\"\"\n    print(\"=\" * 80)\n    print(\"Starting evaluation...\")\n    print(\"=\" * 80)\n    download_nltk_data()\n    model, tokenizer, is_seq2seq = load_model_and_tokenizer(model_path)\n    model.model_type = \"seq2seq\" if is_seq2seq else \"causal\"\n    dev_data = load_pauq_data(DATA_DIR, \"dev\")\n    if num_samples is not None:\n        dev_data = dev_data[:num_samples]\n        print(f\"Evaluating on {num_samples} samples\")\n    else:\n        print(f\"Evaluating on all {len(dev_data)} samples\")\n    print(\"\\\\nGenerating predictions...\")\n    predictions = []\n    references = []\n    for i, item in enumerate(tqdm(dev_data, desc=\"Generating\")):\n        sql_query = item.get(\"query\", {}).get(\"en\", \"\")\n        actual_question = item.get(\"question\", {}).get(\"en\", \"\")\n        if not sql_query or not actual_question:\n            continue\n        predicted_question = generate_question(model, tokenizer, sql_query)\n        predictions.append({\n            \"id\": item.get(\"id\", f\"sample_{i}\"),\n            \"sql\": sql_query,\n            \"reference\": actual_question,\n            \"prediction\": predicted_question,\n        })\n        references.append(actual_question)\n    print(\"\\\\nComputing evaluation metrics...\")\n    hypotheses = [p[\"prediction\"] for p in predictions]\n    bleu_scores = compute_bleu_score(references, hypotheses)\n    rouge_scores = compute_rouge_score(references, hypotheses)\n    chrf_scores = compute_chrf_score(references, hypotheses)\n    model_name = os.path.basename(model_path) if os.path.exists(model_path) else model_path\n    data_count = len(predictions)\n    print(\"\\\\n\" + \"=\" * 80)\n    print(\"Evaluation Results\")\n    print(\"=\" * 80)\n    print(f\"Model: {model_name}\")\n    print(f\"Total samples evaluated: {data_count}\")\n    print(\"\\\\nBLEU Scores:\")\n    for metric, score in bleu_scores.items():\n        print(f\"  {metric}: {score:.2f}\")\n    print(\"\\\\nROUGE Scores:\")\n    for metric, score in rouge_scores.items():\n        print(f\"  {metric}: {score:.2f}\")\n    print(\"\\\\nCHRF Scores:\")\n    for metric, score in chrf_scores.items():\n        print(f\"  {metric}: {score:.2f}\")\n    print(\"=\" * 80)\n    if output_file:\n        print(f\"\\\\nSaving predictions to {output_file}...\")\n        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n            json.dump(predictions, f, indent=2, ensure_ascii=False)\n        print(\"Predictions saved!\")\n    csv_file = os.path.join(OUTPUT_DIR, \"evaluation_results.csv\")\n    all_metrics = {\n        \"model_name\": model_name,\n        \"data_count\": data_count,\n        **bleu_scores,\n        **rouge_scores,\n        **chrf_scores,\n    }\n    print(f\"\\\\nSaving evaluation results to {csv_file}...\")\n    csv_exists = os.path.exists(csv_file)\n    with open(csv_file, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n        writer = csv.DictWriter(f, fieldnames=all_metrics.keys())\n        if not csv_exists:\n            writer.writeheader()\n        writer.writerow(all_metrics)\n    return all_metrics\\n\\n\ndef prepare_datasets_with_split(tokenizer, config, use_dev_as_test: bool = False):\n    \"\"\"Prepare train/validation/test datasets from training data.\n    \n    If use_dev_as_test=True:\n        - Load pauq_train.json and split into train/val\n        - Use pauq_dev.json as test set\n    \n    If use_dev_as_test=False:\n        - Load pauq_train.json for training\n        - Load pauq_dev.json for validation\n    \"\"\"\n    print(\"Loading training data...\")\n    train_data = load_pauq_data(config.data_dir, \"train\")\n    \n    if use_dev_as_test:\n        # Split train data for train/val, use dev as test\n        print(\"Splitting training data into train/val (80/20)...\")\n        train_split, val_split = split_train_data(train_data)\n        \n        print(\"Loading test data (dev set)...\")\n        test_data = load_pauq_data(config.data_dir, \"dev\")\n        \n        train_prepared = prepare_training_data(train_split, is_seq2seq=True)\n        val_prepared = prepare_training_data(val_split, is_seq2seq=True)\n        test_prepared = prepare_training_data(test_data, is_seq2seq=True)\n    else:\n        # Use train for training, dev for validation\n        print(\"Loading validation data...\")\n        val_data = load_pauq_data(config.data_dir, \"dev\")\n        \n        train_prepared = prepare_training_data(train_data, is_seq2seq=True)\n        val_prepared = prepare_training_data(val_data, is_seq2seq=True)\n        test_prepared = []\n        \n    print(f\"Prepared {len(train_prepared)} training examples\")\n    if val_prepared:\n        print(f\"Prepared {len(val_prepared)} validation examples\")\n    if test_prepared:\n        print(f\"Prepared {len(test_prepared)} test examples\")\n    \n    # Create datasets\n    train_dataset = Dataset.from_list(train_prepared)\n    val_dataset = Dataset.from_list(val_prepared) if val_prepared else None\n    test_dataset = Dataset.from_list(test_prepared) if test_prepared else None\n    \n    # Tokenize datasets\n    cols_to_remove = [\"input\", \"target\"]  # Always seq2seq format for RuT5\n    \n    train_dataset = train_dataset.map(\n        lambda x: tokenize_function(x, tokenizer, config.max_length, is_seq2seq=True),\n        batched=True,\n        remove_columns=cols_to_remove,\n    )\n    \n    if val_dataset:\n        val_dataset = val_dataset.map(\n            lambda x: tokenize_function(x, tokenizer, config.max_length, is_seq2seq=True),\n            batched=True,\n            remove_columns=cols_to_remove,\n        )\n    \n    if test_dataset:\n        test_dataset = test_dataset.map(\n            lambda x: tokenize_function(x, tokenizer, config.max_length, is_seq2seq=True),\n            batched=True,\n            remove_columns=cols_to_remove,\n        )\n    \n    return train_dataset, val_dataset, test_dataset\n\n\ndef evaluate_with_test_set(model_path: str, num_samples: Optional[int] = None, output_file: Optional[str] = None) -> Dict:\n    \"\"\"Evaluate model using dev set as test set.\"\"\"\n    print(\"=\" * 80)\n    print(\"Starting evaluation with test set...\")\n    print(\"=\" * 80)\n    \n    download_nltk_data()\n    model, tokenizer, is_seq2seq = load_model_and_tokenizer(model_path)\n    model.model_type = \"seq2seq\" if is_seq2seq else \"causal\"\n    \n    # Load dev as test set\n    test_data = load_pauq_data(DATA_DIR, \"dev\")\n    \n    if num_samples is not None:\n        test_data = test_data[:num_samples]\n        print(f\"Evaluating on {num_samples} samples\")\n    else:\n        print(f\"Evaluating on all {len(test_data)} samples\")\n    \n    # Prepare test data\n    test_prepared = prepare_training_data(test_data, is_seq2seq=True)\n    test_dataset = Dataset.from_list(test_prepared)\n    test_dataset = test_dataset.map(\n        lambda x: tokenize_function(x, tokenizer, 512, is_seq2seq=True),\n        batched=True,\n        remove_columns=[\"input\", \"target\"],\n    )\n    \n    print(\"\\\\nGenerating predictions...\")\n    predictions = []\n    references = []\n    \n    for i, item in enumerate(tqdm(test_dataset, desc=\"Generating\")):\n        sql_query = item.get(\"query\", {}).get(\"en\", \"\")\n        actual_question = item.get(\"question\", {}).get(\"en\", \"\")\n        \n        if not sql_query or not actual_question:\n            continue\n        \n        predicted_question = generate_question(model, tokenizer, sql_query)\n        \n        predictions.append({\n            \"id\": item.get(\"id\", f\"sample_{i}\"),\n            \"sql\": sql_query,\n            \"reference\": actual_question,\n            \"prediction\": predicted_question,\n        })\n        references.append(actual_question)\n    \n    print(\"\\\\nComputing evaluation metrics...\")\n    hypotheses = [p[\"prediction\"] for p in predictions]\n    \n    bleu_scores = compute_bleu_score(references, hypotheses)\n    rouge_scores = compute_rouge_score(references, hypotheses)\n    chrf_scores = compute_chrf_score(references, hypotheses)\n    \n    model_name = os.path.basename(model_path) if os.path.exists(model_path) else model_path\n    data_count = len(predictions)\n    \n    print(\"\\\\n\" + \"=\" * 80)\n    print(\"Evaluation Results\")\n    print(\"=\" * 80)\n    print(f\"Model: {model_name}\")\n    print(f\"Total samples evaluated: {data_count}\")\n    \n    print(\"\\\\nBLEU Scores:\")\n    for metric, score in bleu_scores.items():\n        print(f\"  {metric}: {score:.2f}\")\n    \n    print(\"\\\\nROUGE Scores:\")\n    for metric, score in rouge_scores.items():\n        print(f\"  {metric}: {score:.2f}\")\n    \n    print(\"\\\\nCHRF Scores:\")\n    for metric, score in chrf_scores.items():\n        print(f\"  {metric}: {score:.2f}\")\n    print(\"=\" * 80)\n    \n    if output_file:\n        print(f\"\\\\nSaving predictions to {output_file}...\")\n        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n            json.dump(predictions, f, indent=2, ensure_ascii=False)\n        print(\"Predictions saved!\")\n    \n    csv_file = os.path.join(OUTPUT_DIR, \"evaluation_results.csv\")\n    all_metrics = {\n        \"model_name\": model_name,\n        \"data_count\": data_count,\n        **bleu_scores,\n        **rouge_scores,\n        **chrf_scores,\n    }\n    \n    print(f\"\\\\nSaving evaluation results to {csv_file}...\")\n    csv_exists = os.path.exists(csv_file)\n    with open(csv_file, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n        writer = csv.DictWriter(f, fieldnames=all_metrics.keys())\n        if not csv_exists:\n            writer.writeheader()\n        writer.writerow(all_metrics)\n    \n    return all_metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Set your model and training parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===================== MODEL SELECTION =====================\n# Choose your model from HuggingFace Hub\n#\n# Seq2Seq models (encoder-decoder):\n# - \"cointegrated/rut5-base\" - Russian T5 (default)\n# - \"cointegrated/rut5-small\" - Smaller Russian T5\n# - \"google/flan-t5-base\" - English T5\n# - \"facebook/bart-base\" - BART\n#\n# Causal LMs (decoder-only):\n# - \"Qwen/Qwen2.5-0.5B-Instruct\" - Small Qwen\n# - \"microsoft/Phi-3-mini-4k-instruct\" - Phi-3\n# - \"meta-llama/Llama-3.2-1B-Instruct\" - Llama\n# =========================================================\n\nMODEL_NAME = \"cointegrated/rut5-base\"\n\n# ==================== TRAINING CONFIG ====================\n# Adjust based on your GPU memory\n# Colab T4: batch_size=4, gradient_accumulation=4\n# Colab A100: batch_size=16, gradient_accumulation=2\nBATCH_SIZE = 4\nGRADIENT_ACCUMULATION_STEPS = 4\nNUM_EPOCHS = 3\nLEARNING_RATE = 2e-4\nMAX_LENGTH = 512\n\n# ==================== EVALUATION CONFIG ===================\nNUM_EVAL_SAMPLES = 100  # Set to None to evaluate on all samples\n\nprint(f\"Model: {MODEL_NAME}\")\nprint(f\"Batch size: {BATCH_SIZE}\")\nprint(f\"Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"Epochs: {NUM_EPOCHS}\")\n\nUSE_DEV_AS_TEST = True  # Use train data split + dev as test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check GPU Availability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"No GPU available, using CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Model\n",
        "\n",
        "Configure and run the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure training\nconfig = TrainingConfig(\n    model_name=MODEL_NAME,\n    output_dir=OUTPUT_DIR,\n    data_dir=DATA_DIR,\n    batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    num_epochs=NUM_EPOCHS,\n    learning_rate=LEARNING_RATE,\n    max_length=MAX_LENGTH,\n)\n\n# Run training with split (train/val from train data, dev as test)\ntrain_dataset, val_dataset, test_dataset = prepare_datasets_with_split(\n    tokenizer=None,\n    config=config,\n    use_dev_as_test=USE_DEV_AS_TEST,\n)\n\ntrainer = train_model(config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set (dev set as test)\\n",
        "print(\"=\" * 70)\\n",
        "print(\"EVALUATION MODE: Using dev set as test set\")\\n",
        "print(\"=\" * 70)\\n",
        "\\n",
        "metrics = evaluate_with_test_set(\\n",
        "    model_path=OUTPUT_DIR,\\n",
        "    num_samples=NUM_EVAL_SAMPLES,\\n",
        "    output_file=f\"{OUTPUT_DIR}/predictions.json\"\\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the trained model\n",
        "metrics = evaluate_model(\n",
        "    model_path=OUTPUT_DIR,\n",
        "    num_samples=NUM_EVAL_SAMPLES,\n",
        "    output_file=f\"{OUTPUT_DIR}/predictions.json\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## View Evaluation Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read and display evaluation results\n",
        "results_file = f\"{OUTPUT_DIR}/evaluation_results.csv\"\n",
        "if os.path.exists(results_file):\n",
        "    results_df = pd.read_csv(results_file)\n",
        "    print(\"Evaluation Results:\")\n",
        "    display(results_df)\n",
        "else:\n",
        "    print(\"No evaluation results found. Run evaluation first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model and generate sample predictions\n",
        "model, tokenizer, is_seq2seq = load_model_and_tokenizer(OUTPUT_DIR)\n",
        "model.model_type = \"seq2seq\" if is_seq2seq else \"causal\"\n",
        "\n",
        "# Get a few samples from dev set\n",
        "dev_samples = load_pauq_data(DATA_DIR, \"dev\")[:5]\n",
        "\n",
        "print(\"\\nSample Predictions:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, item in enumerate(dev_samples):\n",
        "    sql_query = item.get(\"query\", {}).get(\"en\", \"\")\n",
        "    actual_question = item.get(\"question\", {}).get(\"en\", \"\")\n",
        "\n",
        "    predicted = generate_question(model, tokenizer, sql_query)\n",
        "    \n",
        "    print(f\"\\n--- Sample {i+1} ---\")\n",
        "    print(f\"SQL: {sql_query}\")\n",
        "    print(f\"Reference: {actual_question}\")\n",
        "    print(f\"Predicted: {predicted}\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Model (Optional)\n",
        "\n",
        "If you want to download the trained model to your local machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Zip the model directory\n",
        "zip_filename = \"sql_to_text_model.zip\"\n",
        "print(f\"Zipping model to {zip_filename}...\")\n",
        "shutil.make_archive(zip_filename.replace('.zip', ''), 'zip', OUTPUT_DIR)\n",
        "print(\"Zipping complete!\")\n",
        "\n",
        "# Download the zip file\n",
        "print(\"\\nStarting download...\")\n",
        "files.download(zip_filename)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}