{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL-to-Text Training and Evaluation (Colab)\n",
    "\n",
    "This notebook fine-tunes and evaluates a model for SQL-to-text generation using the PAUQ dataset.\n",
    "\n",
    "## Supported Model Types\n",
    "- **Seq2Seq models**: RuT5, T5, BART, mBART, etc.\n",
    "- **Causal LMs**: Qwen, Llama, Mistral, etc.\n",
    "\n",
    "The code automatically detects the model type and uses the appropriate training/inference approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch transformers datasets accelerate peft rouge-score nltk sacrebleu sentencepiece tqdm\n",
    "\n",
    "print(\"Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mount Google Drive (Optional)\n",
    "\n",
    "Mount Google Drive to save your trained model and results permanently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set your Google Drive path\n",
    "DRIVE_PATH = \"/content/drive/MyDrive/sql_to_text\"\n",
    "OUTPUT_DIR = f\"{DRIVE_PATH}/model_output\"\n",
    "DATA_DIR = f\"{DRIVE_PATH}/data\"\n",
    "\n",
    "# Create directories\n",
    "!mkdir -p {OUTPUT_DIR}\n",
    "!mkdir -p {DATA_DIR}\n",
    "\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Data Files\n",
    "\n",
    "Upload your PAUQ dataset files (`pauq_train.json` and `pauq_dev.json`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"Please upload pauq_train.json and pauq_dev.json\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Move uploaded files to data directory\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "for filename in uploaded.keys():\n",
    "    if filename.endswith('.json'):\n",
    "        shutil.move(filename, os.path.join(DATA_DIR, filename))\n",
    "        print(f\"Moved {filename} to {DATA_DIR}\")\n",
    "\n",
    "# Verify files\n",
    "train_file = os.path.join(DATA_DIR, \"pauq_train.json\")\n",
    "dev_file = os.path.join(DATA_DIR, \"pauq_dev.json\")\n",
    "\n",
    "if os.path.exists(train_file) and os.path.exists(dev_file):\n",
    "    print(\"\\nData files ready!\")\n",
    "else:\n",
    "    print(\"\\nWarning: Missing data files!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set your model and training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== MODEL SELECTION =====================\n",
    "# Choose your model from HuggingFace Hub\n",
    "#\n",
    "# Seq2Seq models (encoder-decoder):\n",
    "# - \"cointegrated/rut5-base\" - Russian T5 (default)\n",
    "# - \"cointegrated/rut5-small\" - Smaller Russian T5\n",
    "# - \"google/flan-t5-base\" - English T5\n",
    "# - \"facebook/bart-base\" - BART\n",
    "#\n",
    "# Causal LMs (decoder-only):\n",
    "# - \"Qwen/Qwen2.5-0.5B-Instruct\" - Small Qwen\n",
    "# - \"microsoft/Phi-3-mini-4k-instruct\" - Phi-3\n",
    "# - \"meta-llama/Llama-3.2-1B-Instruct\" - Llama\n",
    "# =========================================================\n",
    "\n",
    "MODEL_NAME = \"cointegrated/rut5-base\"\n",
    "\n",
    "# ==================== TRAINING CONFIG ====================\n",
    "# Adjust based on your GPU memory\n",
    "# Colab T4: batch_size=4, gradient_accumulation=4\n",
    "# Colab A100: batch_size=16, gradient_accumulation=2\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 2e-4\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# ==================== EVALUATION CONFIG ===================\n",
    "NUM_EVAL_SAMPLES = 100  # Set to None to evaluate on all samples\n",
    "MAX_NEW_TOKENS = 100\n",
    "TEMPERATURE = 0.7\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU available, using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Script\n",
    "\n",
    "The following script handles both seq2seq and causal LM models automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write training script\n",
    "training_script = '''\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Train a model for SQL-to-text generation.\n",
    "\n",
    "Supports both causal LMs (Qwen, Llama, etc.) and seq2seq models (RuT5/T5).\n",
    "Fine-tunes on PAUQ dataset to generate natural language questions from SQL queries.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    TrainingArguments,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Trainer,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "\n",
    "# Default model\n",
    "DEFAULT_MODEL_NAME = \"cointegrated/rut5-base\"\n",
    "\n",
    "# Training configuration\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 2e-4\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Training hyperparameters.\"\"\"\n",
    "    model_name: str = DEFAULT_MODEL_NAME\n",
    "    output_dir: str = \"./output/sql_to_text\"\n",
    "    data_dir: str = \"./data\"\n",
    "    max_length: int = MAX_LENGTH\n",
    "    batch_size: int = BATCH_SIZE\n",
    "    gradient_accumulation_steps: int = GRADIENT_ACCUMULATION_STEPS\n",
    "    num_epochs: int = NUM_EPOCHS\n",
    "    learning_rate: float = LEARNING_RATE\n",
    "    warmup_steps: int = 100\n",
    "    logging_steps: int = 10\n",
    "    save_steps: int = 100\n",
    "    eval_steps: int = 100\n",
    "\n",
    "\n",
    "def load_pauq_data(data_dir: str, split: str = \"train\") -> List[Dict]:\n",
    "    \"\"\"Load PAUQ dataset from JSON file.\"\"\"\n",
    "    filename = f\"pauq_{split}.json\"\n",
    "    filepath = os.path.join(data_dir, filename)\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"Data file not found: {filepath}\")\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    print(f\"Loaded {len(data)} examples from {filename}\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def prepare_training_data(data: List[Dict], is_seq2seq: bool = False) -> List[Dict]:\n",
    "    \"\"\"Prepare data for fine-tuning.\"\"\"\n",
    "    prepared = []\n",
    "\n",
    "    for item in data:\n",
    "        sql_query = item.get(\"query\", {}).get(\"en\", \"\")\n",
    "        question = item.get(\"question\", {}).get(\"en\", \"\")\n",
    "\n",
    "        if not sql_query or not question:\n",
    "            continue\n",
    "\n",
    "        sql_query = sql_query.strip()\n",
    "        question = question.strip()\n",
    "\n",
    "        if is_seq2seq:\n",
    "            prepared.append({\n",
    "                \"input\": f\"SQL: {sql_query}\",\n",
    "                \"target\": question\n",
    "            })\n",
    "        else:\n",
    "            formatted_text = f\"SQL: {sql_query}\\\\nQuestion: {question}\"\n",
    "            prepared.append({\"text\": formatted_text})\n",
    "\n",
    "    print(f\"Prepared {len(prepared)} training examples\")\n",
    "    return prepared\n",
    "\n",
    "\n",
    "def tokenize_function(examples, tokenizer, max_length, is_seq2seq: bool = False):\n",
    "    \"\"\"Tokenize the text data.\"\"\"\n",
    "    if is_seq2seq:\n",
    "        inputs = tokenizer(\n",
    "            examples[\"input\"],\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        targets = tokenizer(\n",
    "            examples[\"target\"],\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "        labels = inputs[\"labels\"]\n",
    "        labels = [\n",
    "            [(label if label != tokenizer.pad_token_id else -100) for label in labels_seq]\n",
    "            for labels_seq in labels\n",
    "        ]\n",
    "        inputs[\"labels\"] = labels\n",
    "        return inputs\n",
    "    else:\n",
    "        tokenized = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        return tokenized\n",
    "\n",
    "\n",
    "def prepare_datasets(tokenizer, config: TrainingConfig, is_seq2seq: bool = False):\n",
    "    \"\"\"Prepare train and validation datasets.\"\"\"\n",
    "    print(\"Loading training data...\")\n",
    "    train_data = load_pauq_data(config.data_dir, \"train\")\n",
    "    train_prepared = prepare_training_data(train_data, is_seq2seq=is_seq2seq)\n",
    "\n",
    "    print(\"Loading validation data...\")\n",
    "    val_data = load_pauq_data(config.data_dir, \"dev\")\n",
    "    val_prepared = prepare_training_data(val_data, is_seq2seq=is_seq2seq)\n",
    "\n",
    "    train_dataset = Dataset.from_list(train_prepared)\n",
    "    val_dataset = Dataset.from_list(val_prepared)\n",
    "\n",
    "    cols_to_remove = [\"input\", \"target\"] if is_seq2seq else [\"text\"]\n",
    "\n",
    "    train_dataset = train_dataset.map(\n",
    "        lambda x: tokenize_function(x, tokenizer, config.max_length, is_seq2seq=is_seq2seq),\n",
    "        batched=True,\n",
    "        remove_columns=cols_to_remove,\n",
    "    )\n",
    "\n",
    "    val_dataset = val_dataset.map(\n",
    "        lambda x: tokenize_function(x, tokenizer, config.max_length, is_seq2seq=is_seq2seq),\n",
    "        batched=True,\n",
    "        remove_columns=cols_to_remove,\n",
    "    )\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer(model_name: str):\n",
    "    \"\"\"Load model and tokenizer. Returns model, tokenizer, and is_seq2seq flag.\"\"\"\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    try:\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            low_cpu_mem_usage=True,\n",
    "        )\n",
    "        is_seq2seq = True\n",
    "        print(\"Detected: Seq2Seq model (encoder-decoder architecture)\")\n",
    "    except (OSError, ValueError, KeyError):\n",
    "        try:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                low_cpu_mem_usage=True,\n",
    "            )\n",
    "            is_seq2seq = False\n",
    "            print(\"Detected: Causal LM (decoder-only architecture)\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load model {model_name}: {e}\")\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer, is_seq2seq\n",
    "\n",
    "\n",
    "def train_model(config: TrainingConfig):\n",
    "    \"\"\"Main training function.\"\"\"\n",
    "    print(\"Starting SQL-to-text training...\")\n",
    "\n",
    "    model, tokenizer, is_seq2seq = load_model_and_tokenizer(config.model_name)\n",
    "\n",
    "    train_dataset, val_dataset = prepare_datasets(tokenizer, config, is_seq2seq=is_seq2seq)\n",
    "\n",
    "    if is_seq2seq:\n",
    "        data_collator = DataCollatorForSeq2Seq(\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "            padding=True,\n",
    "        )\n",
    "        TrainerClass = Seq2SeqTrainer\n",
    "        TrainingArgsClass = Seq2SeqTrainingArguments\n",
    "    else:\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer,\n",
    "            mlm=False,\n",
    "        )\n",
    "        TrainerClass = Trainer\n",
    "        TrainingArgsClass = TrainingArguments\n",
    "\n",
    "    training_args = TrainingArgsClass(\n",
    "        output_dir=config.output_dir,\n",
    "        num_train_epochs=config.num_epochs,\n",
    "        per_device_train_batch_size=config.batch_size,\n",
    "        per_device_eval_batch_size=config.batch_size,\n",
    "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "        learning_rate=config.learning_rate,\n",
    "        warmup_steps=config.warmup_steps,\n",
    "        logging_steps=config.logging_steps,\n",
    "        save_steps=config.save_steps,\n",
    "        eval_steps=config.eval_steps,\n",
    "        save_total_limit=3,\n",
    "        fp16=False,\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        eval_strategy=\"steps\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        report_to=\"none\",\n",
    "        remove_unused_columns=False,\n",
    "        predict_with_generate=True if is_seq2seq else False,\n",
    "    )\n",
    "\n",
    "    trainer = TrainerClass(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,\n",
    "        processing_class=tokenizer,\n",
    "    )\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "\n",
    "    print(f\"Saving final model to {config.output_dir}\")\n",
    "    trainer.save_model(config.output_dir)\n",
    "    tokenizer.save_pretrained(config.output_dir)\n",
    "\n",
    "    print(\"Training completed!\")\n",
    "    return trainer\n",
    "\n",
    "\n",
    "def evaluate_model(model_path: str, num_samples: int = 5):\n",
    "    \"\"\"Evaluate the trained model with sample predictions.\"\"\"\n",
    "    print(f\"\\\\nEvaluating model: {model_path}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    try:\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        is_seq2seq = True\n",
    "    except (OSError, ValueError, KeyError):\n",
    "        try:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "            )\n",
    "            is_seq2seq = False\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load model from {model_path}: {e}\")\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    val_data = load_pauq_data(config.data_dir, \"dev\")\n",
    "\n",
    "    print(\"\\\\nSample predictions:\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for i, item in enumerate(val_data[:num_samples]):\n",
    "        sql_query = item.get(\"query\", {}).get(\"en\", \"\")\n",
    "        actual_question = item.get(\"question\", {}).get(\"en\", \"\")\n",
    "\n",
    "        prompt = f\"SQL: {sql_query}\\\\nQuestion:\"\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        if is_seq2seq:\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=100,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "            predicted_question = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        else:\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=100,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "            full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            predicted_question = full_output.split(\"Question:\")[-1].strip()\n",
    "\n",
    "        print(f\"\\\\n--- Sample {i+1} ---\")\n",
    "        print(f\"SQL: {sql_query}\")\n",
    "        print(f\"Expected: {actual_question}\")\n",
    "        print(f\"Predicted: {predicted_question}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"Train model for SQL-to-text\")\n",
    "    parser.add_argument(\"--model\", default=DEFAULT_MODEL_NAME)\n",
    "    parser.add_argument(\"--output-dir\", default=\"./output/sql_to_text\")\n",
    "    parser.add_argument(\"--data-dir\", default=\"./data\")\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=BATCH_SIZE)\n",
    "    parser.add_argument(\"--gradient-accumulation-steps\", type=int, default=GRADIENT_ACCUMULATION_STEPS)\n",
    "    parser.add_argument(\"--num-epochs\", type=int, default=NUM_EPOCHS)\n",
    "    parser.add_argument(\"--learning-rate\", type=float, default=LEARNING_RATE)\n",
    "    parser.add_argument(\"--max-length\", type=int, default=MAX_LENGTH)\n",
    "    parser.add_argument(\"--mode\", choices=[\"train\", \"eval\", \"both\"], default=\"both\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    config = TrainingConfig(\n",
    "        model_name=args.model,\n",
    "        output_dir=args.output_dir,\n",
    "        data_dir=args.data_dir,\n",
    "        batch_size=args.batch_size,\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        num_epochs=args.num_epochs,\n",
    "        learning_rate=args.learning_rate,\n",
    "        max_length=args.max_length,\n",
    "    )\n",
    "\n",
    "    if args.mode in [\"train\", \"both\"]:\n",
    "        trainer = train_model(config)\n",
    "\n",
    "    if args.mode in [\"eval\", \"both\"]:\n",
    "        evaluate_model(config.output_dir, num_samples=5)\n",
    "'''\n",
    "\n",
    "# Write the training script to a file\n",
    "with open('train_sql_to_text.py', 'w') as f:\n",
    "    f.write(training_script)\n",
    "\n",
    "print(\"Training script created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write evaluation script\n",
    "evaluation_script = '''\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Evaluate a model on dev set from data folder.\n",
    "\n",
    "This script loads a model and evaluates it on PAUQ dev set,\n",
    "computing metrics like BLEU, ROUGE, CHRF, and LaBSE similarity.\n",
    "Results are saved to a CSV file with model name and data count.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModel,\n",
    ")\n",
    "from rouge_score import rouge_scorer\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import SmoothingFunction, corpus_bleu\n",
    "from tqdm import tqdm\n",
    "from sacrebleu.metrics import CHRF\n",
    "\n",
    "DEFAULT_MODEL_NAME = \"cointegrated/rut5-base\"\n",
    "DATA_DIR = \"./data\"\n",
    "MAX_NEW_TOKENS = 100\n",
    "TEMPERATURE = 0.7\n",
    "DO_SAMPLE = True\n",
    "\n",
    "\n",
    "def download_nltk_data():\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt_tab')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "\n",
    "def load_dev_data(data_dir: str) -> List[Dict]:\n",
    "    filepath = os.path.join(data_dir, \"pauq_dev.json\")\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"Dev data file not found: {filepath}\")\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"Loaded {len(data)} examples from dev set\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer(model_path: str):\n",
    "    print(f\"Loading model from: {model_path}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    try:\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        model.model_type = \"seq2seq\"\n",
    "    except (OSError, ValueError, KeyError):\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        model.model_type = \"causal\"\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def generate_question(model, tokenizer, sql_query: str, max_new_tokens: int, temperature: float, do_sample: bool) -> str:\n",
    "    examples = \"\"\"Generate a natural language question from this SQL query.\n",
    "\n",
    "SQL: SELECT count(*) FROM singer;\n",
    "Question: How many singers do we have?\n",
    "\n",
    "SQL: SELECT name ,  country ,  age FROM singer ORDER BY age DESC;\n",
    "Question: Show the name, country and age of all singers, ordered by age from oldest to youngest.\n",
    "\n",
    "SQL: SELECT name FROM singer WHERE country = 'USA';\n",
    "Question: What are the names of singers from the USA?\n",
    "\n",
    "SQL: SELECT count(*) FROM album WHERE singer_id = 1;\n",
    "Question: How many albums does singer 1 have?\n",
    "\n",
    "SQL: {sql_query}\n",
    "Question:\"\"\"\n",
    "\n",
    "    prompt = examples.format(sql_query=sql_query)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if getattr(model, \"model_type\", None) == \"seq2seq\":\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature if do_sample else None,\n",
    "                do_sample=do_sample,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "            predicted_question = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        else:\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=do_sample,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "            full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            predicted_question = full_output.split(\"Question:\")[-1].strip()\n",
    "\n",
    "    predicted_question = predicted_question.split(\"\\\\n\")[0].strip()\n",
    "    return predicted_question\n",
    "\n",
    "\n",
    "def compute_bleu_score(references: List[str], hypotheses: List[str]) -> Dict[str, float]:\n",
    "    smoothing = SmoothingFunction()\n",
    "    ref_tokens = [nltk.word_tokenize(ref.lower()) for ref in references]\n",
    "    hyp_tokens = [nltk.word_tokenize(hyp.lower()) for hyp in hypotheses]\n",
    "    bleu_scores = {}\n",
    "    for n in range(1, 5):\n",
    "        weights = tuple([1.0/n] * n + [0.0] * (4-n))\n",
    "        try:\n",
    "            score = corpus_bleu(\n",
    "                [[ref] for ref in ref_tokens],\n",
    "                hyp_tokens,\n",
    "                weights=weights,\n",
    "                smoothing_function=smoothing.method1,\n",
    "            )\n",
    "            bleu_scores[f\"BLEU-{n}\"] = score * 100\n",
    "        except Exception:\n",
    "            bleu_scores[f\"BLEU-{n}\"] = 0.0\n",
    "    return bleu_scores\n",
    "\n",
    "\n",
    "def compute_chrf_score(references: List[str], hypotheses: List[str]) -> Dict[str, float]:\n",
    "    chrf = CHRF()\n",
    "    ref_str = \" ||| \".join(refs for refs in references)\n",
    "    hyp_str = \"\\\\n\".join(hypotheses)\n",
    "    result = chrf.corpus_score(hyp_str, [ref_str])\n",
    "    return {\"CHRF\": result.score}\n",
    "\n",
    "\n",
    "def compute_rouge_score(references: List[str], hypotheses: List[str]) -> Dict[str, float]:\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        scores = scorer.score(ref, hyp)\n",
    "        rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "        rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "        rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "    return {\n",
    "        \"ROUGE-1\": sum(rouge1_scores) / len(rouge1_scores) * 100,\n",
    "        \"ROUGE-2\": sum(rouge2_scores) / len(rouge2_scores) * 100,\n",
    "        \"ROUGE-L\": sum(rougeL_scores) / len(rougeL_scores) * 100,\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_model(model_path: str, num_samples: int = None, output_file: str = None):\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Starting evaluation...\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    download_nltk_data()\n",
    "    model, tokenizer = load_model_and_tokenizer(model_path)\n",
    "    dev_data = load_dev_data(DATA_DIR)\n",
    "\n",
    "    if num_samples is not None:\n",
    "        dev_data = dev_data[:num_samples]\n",
    "        print(f\"Evaluating on {num_samples} samples\")\n",
    "    else:\n",
    "        print(f\"Evaluating on all {len(dev_data)} samples\")\n",
    "\n",
    "    print(\"\\\\nGenerating predictions...\")\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for i, item in enumerate(tqdm(dev_data, desc=\"Generating\")):\n",
    "        sql_query = item.get(\"query\", {}).get(\"en\", \"\")\n",
    "        actual_question = item.get(\"question\", {}).get(\"en\", \"\")\n",
    "\n",
    "        if not sql_query or not actual_question:\n",
    "            continue\n",
    "\n",
    "        predicted_question = generate_question(model, tokenizer, sql_query, MAX_NEW_TOKENS, TEMPERATURE, DO_SAMPLE)\n",
    "\n",
    "        predictions.append({\n",
    "            \"id\": item.get(\"id\", f\"sample_{i}\"),\n",
    "            \"sql\": sql_query,\n",
    "            \"reference\": actual_question,\n",
    "            \"prediction\": predicted_question,\n",
    "        })\n",
    "        references.append(actual_question)\n",
    "\n",
    "    print(\"\\\\nComputing evaluation metrics...\")\n",
    "    hypotheses = [p[\"prediction\"] for p in predictions]\n",
    "\n",
    "    bleu_scores = compute_bleu_score(references, hypotheses)\n",
    "    rouge_scores = compute_rouge_score(references, hypotheses)\n",
    "    chrf_scores = compute_chrf_score(references, hypotheses)\n",
    "\n",
    "    model_name = os.path.basename(model_path) if os.path.exists(model_path) else model_path\n",
    "    data_count = len(predictions)\n",
    "\n",
    "    print(\"\\\\n\" + \"=\" * 80)\n",
    "    print(\"Evaluation Results\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Total samples evaluated: {data_count}\")\n",
    "\n",
    "    print(\"\\\\nBLEU Scores:\")\n",
    "    for metric, score in bleu_scores.items():\n",
    "        print(f\"  {metric}: {score:.2f}\")\n",
    "\n",
    "    print(\"\\\\nROUGE Scores:\")\n",
    "    for metric, score in rouge_scores.items():\n",
    "        print(f\"  {metric}: {score:.2f}\")\n",
    "\n",
    "    print(\"\\\\nCHRF Scores:\")\n",
    "    for metric, score in chrf_scores.items():\n",
    "        print(f\"  {metric}: {score:.2f}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    if output_file:\n",
    "        print(f\"\\\\nSaving predictions to {output_file}...\")\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(predictions, f, indent=2, ensure_ascii=False)\n",
    "        print(\"Predictions saved!\")\n",
    "\n",
    "    csv_file = \"evaluation_results.csv\"\n",
    "    all_metrics = {\n",
    "        \"model_name\": model_name,\n",
    "        \"data_count\": data_count,\n",
    "        **bleu_scores,\n",
    "        **rouge_scores,\n",
    "        **chrf_scores,\n",
    "    }\n",
    "\n",
    "    print(f\"\\\\nSaving evaluation results to {csv_file}...\")\n",
    "    csv_exists = os.path.exists(csv_file)\n",
    "    with open(csv_file, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=all_metrics.keys())\n",
    "        if not csv_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(all_metrics)\n",
    "    return all_metrics\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Evaluate model on PAUQ dev set\")\n",
    "    parser.add_argument(\"--model-path\", type=str, default=DEFAULT_MODEL_NAME)\n",
    "    parser.add_argument(\"--num-samples\", type=int, default=None)\n",
    "    parser.add_argument(\"--data-dir\", type=str, default=\"./data\")\n",
    "    args = parser.parse_args()\n",
    "    DATA_DIR = args.data_dir\n",
    "    evaluate_model(args.model_path, args.num_samples)\n",
    "'''\n",
    "\n",
    "# Write the evaluation script to a file\n",
    "with open('evaluate_model.py', 'w') as f:\n",
    "    f.write(evaluation_script)\n",
    "\n",
    "print(\"Evaluation script created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "Run the training script with your configured parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "!python train_sql_to_text.py \\\\\n",
    "    --model {MODEL_NAME} \\\\\n",
    "    --output-dir {OUTPUT_DIR} \\\\\n",
    "    --data-dir {DATA_DIR} \\\\\n",
    "    --batch-size {BATCH_SIZE} \\\\\n",
    "    --gradient-accumulation-steps {GRADIENT_ACCUMULATION_STEPS} \\\\\n",
    "    --num-epochs {NUM_EPOCHS} \\\\\n",
    "    --learning-rate {LEARNING_RATE} \\\\\n",
    "    --max-length {MAX_LENGTH} \\\\\n",
    "    --mode train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "\n",
    "Evaluate the trained model on the dev set with various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "NUM_SAMPLES_TO_EVAL = NUM_EVAL_SAMPLES  # Can be set to None for full evaluation\n",
    "\n",
    "!python evaluate_model.py \\\\\n",
    "    --model-path {OUTPUT_DIR} \\\\\n",
    "    --data-dir {DATA_DIR} \\\\\n",
    "    --num-samples {NUM_SAMPLES_TO_EVAL}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Evaluation Results\n",
    "\n",
    "Load and display the evaluation results from the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read evaluation results\n",
    "results_df = pd.read_csv(f\"{OUTPUT_DIR}/evaluation_results.csv\") if os.path.exists(f\"{OUTPUT_DIR}/evaluation_results.csv\") else pd.read_csv(\"evaluation_results.csv\")\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "display(results_df)\n",
    "\n",
    "# Save to Google Drive if mounted\n",
    "if 'DRIVE_PATH' in locals():\n",
    "    results_df.to_csv(f\"{DRIVE_PATH}/evaluation_results.csv\", index=False)\n",
    "    print(f\"\\nResults saved to {DRIVE_PATH}/evaluation_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Model (Optional)\n",
    "\n",
    "If you want to download the trained model to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the model directory\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "zip_filename = \"sql_to_text_model.zip\"\n",
    "print(f\"Zipping model to {zip_filename}...\")\n",
    "shutil.make_archive(zip_filename.replace('.zip', ''), 'zip', OUTPUT_DIR)\n",
    "print(\"Zipping complete!\")\n",
    "\n",
    "# Download the zip file\n",
    "from google.colab import files\n",
    "print(\"\\nStarting download...\")\n",
    "files.download(zip_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
